"""
Crawler Ä‘á»ƒ crawl dá»¯ liá»‡u tá»« gaigu1.net/gai-goi
Sá»­ dá»¥ng Playwright Ä‘á»ƒ crawl dá»¯ liá»‡u Ä‘áº§y Ä‘á»§
"""

import asyncio
import json
import os
import re
import random
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Browser, Page

class GirlCrawler:
    def __init__(self, max_concurrent: int = 3, delay_min: float = 2.0, delay_max: float = 5.0):
        """
        Args:
            max_concurrent: Sá»‘ lÆ°á»£ng requests Ä‘á»“ng thá»i tá»‘i Ä‘a (máº·c Ä‘á»‹nh: 3)
            delay_min: Delay tá»‘i thiá»ƒu giá»¯a cÃ¡c requests (giÃ¢y)
            delay_max: Delay tá»‘i Ä‘a giá»¯a cÃ¡c requests (giÃ¢y)
        """
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.base_url = 'https://gaigu1.net/gai-goi'
        self.max_concurrent = max_concurrent
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.file_lock = asyncio.Lock()  # Lock Ä‘á»ƒ Ä‘áº£m báº£o thread-safe khi ghi file
        
    async def init_browser(self):
        """Khá»Ÿi táº¡o browser"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        return self.browser
    
    async def close_browser(self):
        """ÄÃ³ng browser"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi Ä‘Ã³ng browser: {e}")
    
    async def crawl_girls_list(self, page_number: int = 1, limit: int = 60) -> List[Dict]:
        """Crawl danh sÃ¡ch girls tá»« trang listing"""
        if not self.browser:
            await self.init_browser()
        
        page = await self.browser.new_page()
        
        # Set user agent Ä‘á»ƒ trÃ¡nh bá»‹ block
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        girls = []
        
        try:
            print(f"ğŸ” Äang crawl trang {page_number}...")
            
            # Navigate to page
            url = f"{self.base_url}?page={page_number}" if page_number > 1 else self.base_url
            print(f"ğŸ“¡ Äang truy cáº­p: {url}")
            
            # Wait for full page load (including all resources)
            await page.goto(url, wait_until='networkidle', timeout=60000)
            
            # Wait a bit more for any lazy-loaded content
            await page.wait_for_timeout(3000)
            
            # Scroll to trigger lazy loading if any
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(2000)
            
            # Scroll back up
            await page.evaluate("window.scrollTo(0, 0)")
            await page.wait_for_timeout(1000)
            
            # Debug: Check page content
            page_title = await page.title()
            print(f"ğŸ“„ TiÃªu Ä‘á» trang: {page_title}")
            
            # Check if data is loaded in HTML
            cards_count = await page.evaluate("document.querySelectorAll('div.list-escorts').length")
            print(f"ğŸ“‹ TÃ¬m tháº¥y {cards_count} cards trÃªn trang")
            
            # Try to find any images on the page
            img_count = await page.evaluate("document.querySelectorAll('img').length")
            print(f"ğŸ–¼ï¸  TÃ¬m tháº¥y {img_count} áº£nh trÃªn trang")
            
            # Check if there are any API calls being made (for debugging)
            # Note: This website uses SSR, so data is already in HTML
            
            # Extract girls data - using actual HTML structure
            girls = await page.evaluate("""
                () => {
                    const results = [];
                    
                    // Find all profile cards using the actual class structure
                    const cards = document.querySelectorAll('div.list-escorts');
                    console.log('Total cards found:', cards.length);
                    
                    cards.forEach((card) => {
                        try {
                            const girl = {
                                name: '',
                                images: [],
                                tags: [],
                                isAvailable: true,
                                location: '',
                                province: '',
                                rating: 0,
                                totalReviews: 0,
                                verified: false,
                                bio: '',
                                age: null,
                                price: '',
                                detailUrl: '',
                                views: 0
                            };
                            
                            // Extract detail URL from the main link
                            const mainLink = card.querySelector('a[href*="/gai-goi/"]');
                            if (mainLink) {
                                const href = mainLink.getAttribute('href');
                                if (href) {
                                    girl.detailUrl = href.startsWith('http') ? href : 'https://gaigu1.net' + href;
                                }
                            }
                            
                            // Extract name from content-title
                            const nameEl = card.querySelector('.content-title');
                            if (nameEl) {
                                girl.name = nameEl.textContent?.trim() || '';
                            }
                            
                            // Extract images - only from the card, not sidebar
                            const imgEl = card.querySelector('img.img-escort-res');
                            if (imgEl) {
                                let src = imgEl.getAttribute('src') || imgEl.src;
                                if (src) {
                                    if (src.startsWith('//')) {
                                        src = 'https:' + src;
                                    } else if (src.startsWith('/')) {
                                        src = 'https://gaigu1.net' + src;
                                    } else if (!src.startsWith('http')) {
                                        src = 'https://gaigu1.net/' + src;
                                    }
                                    girl.images.push(src);
                                }
                            }
                            
                            // Extract location from es-city
                            const locationEl = card.querySelector('.es-city a');
                            if (locationEl) {
                                girl.location = locationEl.textContent?.trim() || '';
                                // Try to extract province from location link
                                const locationHref = locationEl.getAttribute('href') || '';
                                if (locationHref.includes('/sai-gon/')) {
                                    girl.province = 'SÃ i GÃ²n';
                                } else if (locationHref.includes('/ha-noi/')) {
                                    girl.province = 'HÃ  Ná»™i';
                                } else if (locationHref.includes('/da-nang/')) {
                                    girl.province = 'ÄÃ  Náºµng';
                                } else if (locationHref.includes('/binh-duong/')) {
                                    girl.province = 'BÃ¬nh DÆ°Æ¡ng';
                                } else if (locationHref.includes('/dong-nai/')) {
                                    girl.province = 'Äá»“ng Nai';
                                }
                            }
                            
                            // Extract price from left-price
                            const priceEl = card.querySelector('.left-price');
                            if (priceEl) {
                                const priceText = priceEl.textContent?.trim() || '';
                                // Extract price (e.g., "600K" from "600K" or " 600K")
                                const priceMatch = priceText.match(/(\\d+[.,]?\\d*\\s*K|\\d+[.,]?\\d*\\s*tr)/i);
                                if (priceMatch) {
                                    girl.price = priceMatch[1].trim();
                                }
                            }
                            
                            // Extract rating from content-rating
                            const ratingEl = card.querySelector('.content-rating');
                            if (ratingEl) {
                                const ratingText = ratingEl.textContent || '';
                                // Count filled stars (not white/gray)
                                const filledStars = ratingEl.querySelectorAll('i.fa-star:not(.white), i.fa-star[style*="color:"]:not([style*="#909090"])').length;
                                // Or extract from text like "(0)" or "(2)"
                                const reviewMatch = ratingText.match(/\\((\\d+)\\)/);
                                if (reviewMatch) {
                                    girl.totalReviews = parseInt(reviewMatch[1]);
                                }
                                // If we have filled stars, use that as rating
                                if (filledStars > 0) {
                                    girl.rating = filledStars;
                                }
                            }
                            
                            // Extract views from viewed-in
                            const viewsEl = card.querySelector('.viewed-in');
                            if (viewsEl) {
                                const viewsText = viewsEl.textContent?.trim() || '';
                                // Extract views (e.g., "1.2K" -> 1200, "8.7K" -> 8700)
                                const viewsMatch = viewsText.match(/(\\d+[.,]?\\d*)\\s*K/i);
                                if (viewsMatch) {
                                    const num = parseFloat(viewsMatch[1].replace(',', '.'));
                                    girl.views = Math.round(num * 1000);
                                } else {
                                    const numMatch = viewsText.match(/(\\d+)/);
                                    if (numMatch) {
                                        girl.views = parseInt(numMatch[1]);
                                    }
                                }
                            }
                            
                            // Check verified status - look for verified badge or label
                            const verifiedEl = card.querySelector('.label-public, [class*="verified"], [class*="check"]');
                            if (verifiedEl) {
                                girl.verified = true;
                            }
                            
                            // Extract tags if available (might be in a separate section)
                            const tagEls = card.querySelectorAll('[class*="tag"], .hashtag, a[href*="tag"]');
                            tagEls.forEach(tag => {
                                const tagText = tag.textContent?.trim();
                                if (tagText && tagText.length > 0 && tagText.length < 50) {
                                    if (!girl.tags.includes(tagText)) {
                                        girl.tags.push(tagText);
                                    }
                                }
                            });
                            
                            // Only add if we have at least name and image
                            if (girl.name && girl.name.length > 0 && girl.images.length > 0) {
                                // Filter out invalid names (like "SÃ i GÃ²n", "Tags phá»• biáº¿n")
                                const invalidNames = ['SÃ i GÃ²n', 'HÃ  Ná»™i', 'BÃ¬nh DÆ°Æ¡ng', 'ÄÃ  Náºµng', 'Äá»“ng Nai', 
                                                    'Tags phá»• biáº¿n', 'GÃ¡i gá»i', 'Gaigu', 'Gaigoi'];
                                if (!invalidNames.includes(girl.name)) {
                                    results.push(girl);
                                }
                            }
                        } catch (e) {
                            console.error('Error extracting girl:', e);
                        }
                    });
                    
                    console.log('Total girls extracted:', results.length);
                    return results;
                }
            """)
            
            print(f"âœ… ÄÃ£ crawl Ä‘Æ°á»£c {len(girls)} girls tá»« trang {page_number}")
            
            # Debug: Print sample if found
            if len(girls) > 0:
                print(f"ğŸ“ Sample: {girls[0].get('name', 'N/A')} - {len(girls[0].get('images', []))} áº£nh")
            else:
                # Try to get page HTML structure for debugging
                html_snippet = await page.evaluate("""
                    () => {
                        const body = document.body.innerHTML.substring(0, 1000);
                        return body;
                    }
                """)
                print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u. HTML snippet: {html_snippet[:200]}...")
            
            return girls[:limit]
            
        except Exception as e:
            print(f"âŒ Lá»—i khi crawl trang {page_number}: {str(e)}")
            return []
        finally:
            await page.close()
    
    async def crawl_girl_detail(self, url: str) -> Optional[Dict]:
        """Crawl thÃ´ng tin chi tiáº¿t tá»« trang detail"""
        if not self.browser:
            await self.init_browser()
        
        page = await self.browser.new_page()
        
        # Set realistic headers to avoid bot detection
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://gaigu1.net/gai-goi',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1'
        })
        
        try:
            print(f"ğŸ” Äang crawl detail: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(3000)
            
            # Check if blocked by captcha
            page_content = await page.content()
            if 'verify' in page_content.lower() and 'human' in page_content.lower():
                print("âš ï¸  PhÃ¡t hiá»‡n captcha, Ä‘á»£i thÃªm 5 giÃ¢y...")
                await page.wait_for_timeout(5000)
                # Try to wait for content to load
                try:
                    await page.wait_for_selector('.attributes, h1, [class*="gallery"]', timeout=10000)
                except:
                    pass
            
            # Scroll to load lazy images
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(2000)
            
            girl = await page.evaluate("""
                () => {
                    const data = {
                        name: '',
                        images: [],
                        tags: [],
                        bio: '',
                        location: '',
                        province: '',
                        rating: 0,
                        totalReviews: 0,
                        verified: false,
                        age: null,
                        price: '',
                        phone: '',
                        password: '',
                        birthYear: null,
                        height: '',
                        weight: '',
                        measurements: '',
                        origin: '',
                        address: '',
                        workingHours: '',
                        services: []
                    };
                    
                    // Extract name - try multiple selectors
                    const nameSelectors = ['h1', 'h2', '.content-title', '[class*="title"]', '[class*="name"]'];
                    for (const selector of nameSelectors) {
                        const nameEl = document.querySelector(selector);
                        if (nameEl) {
                            const nameText = nameEl.textContent?.trim();
                            if (nameText && nameText.length > 2) {
                                data.name = nameText;
                                break;
                            }
                        }
                    }
                    
                    // Extract all images from gallery using XPath: /html/body/div[7]/div[3]
                    let galleryContainer = null;
                    
                    // Try XPath first: /html/body/div[7]/div[3] (gallery container)
                    try {
                        const xpathResult = document.evaluate('/html/body/div[7]/div[3]', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null);
                        galleryContainer = xpathResult.singleNodeValue;
                    } catch (e) {
                        console.log('XPath not available, trying selectors');
                    }
                    
                    // If XPath doesn't work, try common selectors
                    if (!galleryContainer) {
                        const selectors = [
                            '.gallery',
                            '.photo-gallery',
                            '[class*="gallery"]',
                            '[class*="photo"]',
                            '.thumb-overlay',
                            '.preview',
                            '[id*="preview"]',
                            '.main-image',
                            '.image-gallery'
                        ];
                        for (const selector of selectors) {
                            const el = document.querySelector(selector);
                            if (el && el.querySelectorAll('img').length > 0) {
                                galleryContainer = el;
                                break;
                            }
                        }
                    }
                    
                    // Extract images from gallery container
                    if (galleryContainer) {
                        galleryContainer.querySelectorAll('img').forEach(img => {
                            let src = img.src || 
                                     img.getAttribute('src') || 
                                     img.getAttribute('data-src') || 
                                     img.getAttribute('data-lazy-src') ||
                                     img.getAttribute('data-original') ||
                                     img.getAttribute('data-lazy');
                            if (src) {
                                const lowerSrc = src.toLowerCase();
                                if (!lowerSrc.includes('placeholder') && 
                                    !lowerSrc.includes('logo') && 
                                    !lowerSrc.includes('icon') &&
                                    !lowerSrc.includes('avatar') &&
                                    !lowerSrc.includes('favicon') &&
                                    !lowerSrc.includes('banner')) {
                                    if (src.startsWith('//')) {
                                        src = 'https:' + src;
                                    } else if (src.startsWith('/')) {
                                        src = 'https://gaigu1.net' + src;
                                    } else if (!src.startsWith('http')) {
                                        src = 'https://gaigu1.net/' + src;
                                    }
                                    if (!data.images.includes(src)) {
                                        data.images.push(src);
                                    }
                                }
                            }
                        });
                    }
                    
                    // If still no images, try to find all images that contain photo/tmb/media in path
                    if (data.images.length === 0) {
                        document.querySelectorAll('img').forEach(img => {
                            let src = img.src || img.getAttribute('src') || img.getAttribute('data-src');
                            if (src) {
                                const lowerSrc = src.toLowerCase();
                                // Only include images that look like photos (contain photo, tmb, media)
                                if ((lowerSrc.includes('photo') || lowerSrc.includes('tmb') || lowerSrc.includes('media')) &&
                                    !lowerSrc.includes('placeholder') && 
                                    !lowerSrc.includes('logo') && 
                                    !lowerSrc.includes('icon') && 
                                    !lowerSrc.includes('avatar') && 
                                    !lowerSrc.includes('favicon') &&
                                    !lowerSrc.includes('banner')) {
                                    if (src.startsWith('//')) {
                                        src = 'https:' + src;
                                    } else if (src.startsWith('/')) {
                                        src = 'https://gaigu1.net' + src;
                                    } else if (!src.startsWith('http')) {
                                        src = 'https://gaigu1.net/' + src;
                                    }
                                    if (!data.images.includes(src)) {
                                        data.images.push(src);
                                    }
                                }
                            }
                        });
                    }
                    
                    // Extract attributes from .attributes section
                    // Try XPath first: /html/body/div[5]/div[6]/div[3]/div[1]/div[2] (attributes container)
                    let attributesSection = null;
                    try {
                        const xpathResult = document.evaluate('/html/body/div[5]/div[6]/div[3]/div[1]/div[2]', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null);
                        attributesSection = xpathResult.singleNodeValue;
                    } catch (e) {
                        // Fallback to selector
                        attributesSection = document.querySelector('.attributes');
                    }
                    
                    if (attributesSection) {
                        // Extract price
                        const priceRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'GiÃ¡'
                        );
                        if (priceRow) {
                            const priceValue = priceRow.nextElementSibling;
                            if (priceValue) {
                                data.price = priceValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract phone
                        const phoneRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Sá»‘ Ä‘iá»‡n thoáº¡i'
                        );
                        if (phoneRow) {
                            const phoneValue = phoneRow.nextElementSibling;
                            if (phoneValue) {
                                const phoneLink = phoneValue.querySelector('a[href^="tel:"]');
                                if (phoneLink) {
                                    data.phone = phoneLink.textContent?.trim() || phoneLink.getAttribute('href')?.replace('tel:', '') || '';
                                } else {
                                    data.phone = phoneValue.textContent?.trim() || '';
                                }
                            }
                        }
                        
                        // Extract password
                        const passRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Pass'
                        );
                        if (passRow) {
                            const passValue = passRow.nextElementSibling;
                            if (passValue) {
                                data.password = passValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract birth year
                        const birthRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'NÄƒm sinh'
                        );
                        if (birthRow) {
                            const birthValue = birthRow.nextElementSibling;
                            if (birthValue) {
                                const birthText = birthValue.textContent?.trim() || '';
                                const yearMatch = birthText.match(/(\\d{4})/);
                                if (yearMatch) {
                                    data.birthYear = parseInt(yearMatch[1]);
                                    // Calculate age
                                    const currentYear = new Date().getFullYear();
                                    data.age = currentYear - data.birthYear;
                                }
                            }
                        }
                        
                        // Extract height
                        const heightRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Chiá»u cao'
                        );
                        if (heightRow) {
                            const heightValue = heightRow.nextElementSibling;
                            if (heightValue) {
                                data.height = heightValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract weight
                        const weightRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'CÃ¢n náº·ng'
                        );
                        if (weightRow) {
                            const weightValue = weightRow.nextElementSibling;
                            if (weightValue) {
                                data.weight = weightValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract measurements (3 vÃ²ng)
                        const measureRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Sá»‘ Ä‘o 3 vÃ²ng'
                        );
                        if (measureRow) {
                            const measureValue = measureRow.nextElementSibling;
                            if (measureValue) {
                                data.measurements = measureValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract origin
                        const originRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Xuáº¥t xá»©'
                        );
                        if (originRow) {
                            const originValue = originRow.nextElementSibling;
                            if (originValue) {
                                data.origin = originValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract location/area
                        const areaRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Khu vá»±c'
                        );
                        if (areaRow) {
                            const areaValue = areaRow.nextElementSibling;
                            if (areaValue) {
                                const areaText = areaValue.textContent?.trim() || '';
                                data.location = areaText;
                                // Extract province
                                if (areaText.includes('SÃ i GÃ²n') || areaText.includes('Há»“ ChÃ­ Minh')) {
                                    data.province = 'SÃ i GÃ²n';
                                } else if (areaText.includes('HÃ  Ná»™i')) {
                                    data.province = 'HÃ  Ná»™i';
                                } else if (areaText.includes('ÄÃ  Náºµng')) {
                                    data.province = 'ÄÃ  Náºµng';
                                } else if (areaText.includes('BÃ¬nh DÆ°Æ¡ng')) {
                                    data.province = 'BÃ¬nh DÆ°Æ¡ng';
                                } else if (areaText.includes('Äá»“ng Nai')) {
                                    data.province = 'Äá»“ng Nai';
                                }
                            }
                        }
                        
                        // Extract address
                        const addressRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Äá»‹a chá»‰'
                        );
                        if (addressRow) {
                            const addressValue = addressRow.nextElementSibling;
                            if (addressValue) {
                                data.address = addressValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract working hours
                        const workRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'LÃ m viá»‡c'
                        );
                        if (workRow) {
                            const workValue = workRow.nextElementSibling;
                            if (workValue) {
                                data.workingHours = workValue.textContent?.trim() || '';
                            }
                        }
                        
                        // Extract services
                        const serviceRow = Array.from(attributesSection.querySelectorAll('.col-md-4')).find(el => 
                            el.textContent?.trim() === 'Dá»‹ch vá»¥'
                        );
                        if (serviceRow) {
                            const serviceValue = serviceRow.nextElementSibling;
                            if (serviceValue) {
                                const serviceSpans = serviceValue.querySelectorAll('span, .a-attr span');
                                serviceSpans.forEach(span => {
                                    const serviceText = span.textContent?.trim();
                                    if (serviceText) {
                                        data.services.push(serviceText);
                                    }
                                });
                            }
                        }
                    }
                    
                    // Extract bio/description
                    const bioEl = document.querySelector('[class*="bio"], [class*="description"], [class*="content"] p, .content p');
                    if (bioEl) {
                        data.bio = bioEl.textContent?.trim() || '';
                    }
                    
                    // Extract rating
                    const ratingEl = document.querySelector('[class*="rating"], .content-rating');
                    if (ratingEl) {
                        const ratingText = ratingEl.textContent || '';
                        const reviewMatch = ratingText.match(/\\((\\d+)\\)/);
                        if (reviewMatch) {
                            data.totalReviews = parseInt(reviewMatch[1]);
                        }
                        // Count filled stars
                        const filledStars = ratingEl.querySelectorAll('i.fa-star:not(.white), i.fa-star[style*="color:"]:not([style*="#909090"])').length;
                        if (filledStars > 0) {
                            data.rating = filledStars;
                        }
                    }
                    
                    // Extract verified
                    data.verified = !!document.querySelector('[class*="verified"], [class*="check"], .label-public');
                    
                    // Extract tags
                    document.querySelectorAll('[class*="tag"], .hashtag, a[href*="tag"]').forEach(tag => {
                        const tagText = tag.textContent?.trim();
                        if (tagText) {
                            data.tags.push(tagText);
                        }
                    });
                    
                    return data;
                }
            """)
            
            print(f"âœ… ÄÃ£ crawl detail: {girl.get('name', 'N/A')} - {len(girl.get('images', []))} áº£nh")
            return girl
            
        except Exception as e:
            print(f"âŒ Lá»—i khi crawl detail {url}: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            await page.close()
    
    def sanitize_filename(self, name: str) -> str:
        """Chuyá»ƒn tÃªn gÃ¡i thÃ nh filename há»£p lá»‡"""
        # Loáº¡i bá» emoji vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t
        name = re.sub(r'[^\w\s-]', '', name)
        # Thay khoáº£ng tráº¯ng báº±ng dáº¥u gáº¡ch dÆ°á»›i
        name = re.sub(r'\s+', '_', name)
        # Loáº¡i bá» dáº¥u gáº¡ch dÆ°á»›i liÃªn tiáº¿p
        name = re.sub(r'_+', '_', name)
        # Loáº¡i bá» dáº¥u gáº¡ch dÆ°á»›i á»Ÿ Ä‘áº§u vÃ  cuá»‘i
        name = name.strip('_')
        # Giá»›i háº¡n Ä‘á»™ dÃ i
        if len(name) > 100:
            name = name[:100]
        # Náº¿u rá»—ng, dÃ¹ng tÃªn máº·c Ä‘á»‹nh
        if not name:
            name = "unknown"
        return name
    
    def save_to_json(self, girls: List[Dict], filename: str = None) -> Dict:
        """LÆ°u vÃ o file JSON"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawled_girls_{timestamp}.json"
        
        # Táº¡o thÆ° má»¥c data náº¿u chÆ°a cÃ³
        data_dir = os.path.join(os.path.dirname(__file__), "data")
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(girls, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(girls)} girls vÃ o {filepath}")
        return {"saved": len(girls), "file": filepath}
    
    def save_girl_detail_to_file(self, girl: Dict) -> str:
        """LÆ°u detail cá»§a 1 gÃ¡i vÃ o file riÃªng vá»›i tÃªn lÃ  tÃªn gÃ¡i"""
        if not girl.get('name'):
            return None
        
        # Sanitize tÃªn Ä‘á»ƒ lÃ m filename
        filename = self.sanitize_filename(girl['name'])
        filename = f"{filename}.json"
        
        # Táº¡o thÆ° má»¥c details náº¿u chÆ°a cÃ³
        data_dir = os.path.join(os.path.dirname(__file__), "data", "details")
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(girl, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    async def crawl_and_save(self, page_number: int = 1, limit: int = 60, crawl_details: bool = False):
        """Crawl vÃ  lÆ°u dá»¯ liá»‡u vÃ o JSON
        
        Strategy:
        1. Crawl listing page Ä‘á»ƒ láº¥y danh sÃ¡ch girls vá»›i detailUrl
        2. Náº¿u crawl_details=True: Crawl detail page cho má»—i girl
        """
        # Giai Ä‘oáº¡n 1: Crawl listing page
        girls = await self.crawl_girls_list(page_number, limit)
        
        # Giai Ä‘oáº¡n 2: Crawl detail pages náº¿u Ä‘Æ°á»£c yÃªu cáº§u
        if crawl_details:
            print(f"\n{'='*50}")
            print(f"ğŸ” GIAI ÄOáº N 2: Crawl detail cho {len(girls)} girls")
            print(f"{'='*50}\n")
            
            for i, girl in enumerate(girls, 1):
                if girl.get('detailUrl'):
                    print(f"[{i}/{len(girls)}] Äang crawl detail: {girl.get('name', 'N/A')[:30]}...")
                    detail_data = await self.crawl_girl_detail(girl['detailUrl'])
                    if detail_data:
                        # Merge detail data into girl data (detail data cÃ³ priority)
                        # Giá»¯ láº¡i detailUrl tá»« listing
                        detail_url = girl.get('detailUrl')
                        girl.update(detail_data)
                        girl['detailUrl'] = detail_url  # Äáº£m báº£o giá»¯ láº¡i detailUrl
                    else:
                        print(f"âš ï¸  KhÃ´ng crawl Ä‘Æ°á»£c detail cho: {girl.get('name', 'N/A')}")
                    
                    # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block (2-3 giÃ¢y)
                    if i < len(girls):
                        await asyncio.sleep(2)
                else:
                    print(f"âš ï¸  Girl {i} khÃ´ng cÃ³ detailUrl, bá» qua")
        
        result = self.save_to_json(girls)
        
        return {
            "crawled": len(girls),
            "saved": result.get("saved", 0),
            "file": result.get("file", "")
        }
    
    async def crawl_multiple_pages(self, start_page: int = 1, end_page: int = 5, crawl_details: bool = False):
        """Crawl nhiá»u trang vÃ  lÆ°u vÃ o má»™t file JSON
        
        Strategy:
        1. Crawl táº¥t cáº£ listing pages trÆ°á»›c (nhanh)
        2. Sau Ä‘Ã³ crawl detail cho táº¥t cáº£ girls (náº¿u crawl_details=True)
        """
        all_girls = []
        
        # Giai Ä‘oáº¡n 1: Crawl táº¥t cáº£ listing pages
        print(f"\n{'='*50}")
        print(f"ğŸ“‹ GIAI ÄOáº N 1: Crawl listing pages {start_page} Ä‘áº¿n {end_page}")
        print(f"{'='*50}\n")
        
        for page_num in range(start_page, end_page + 1):
            print(f"\nğŸ“„ Trang {page_num}/{end_page}")
            girls = await self.crawl_girls_list(page_num, 60)
            all_girls.extend(girls)
            print(f"âœ… ÄÃ£ cÃ³ tá»•ng cá»™ng {len(all_girls)} girls\n")
            
            # Delay between pages
            if page_num < end_page:
                print(f"â³ Äá»£i 5 giÃ¢y trÆ°á»›c khi crawl trang tiáº¿p theo...\n")
                await asyncio.sleep(5)
        
        # Giai Ä‘oáº¡n 2: Crawl detail náº¿u Ä‘Æ°á»£c yÃªu cáº§u
        if crawl_details:
            print(f"\n{'='*50}")
            print(f"ğŸ” GIAI ÄOáº N 2: Crawl detail cho {len(all_girls)} girls")
            print(f"{'='*50}\n")
            
            for i, girl in enumerate(all_girls, 1):
                if girl.get('detailUrl'):
                    print(f"[{i}/{len(all_girls)}] Äang crawl detail: {girl.get('name', 'N/A')[:40]}...")
                    detail_data = await self.crawl_girl_detail(girl['detailUrl'])
                    if detail_data:
                        detail_url = girl.get('detailUrl')
                        girl.update(detail_data)
                        girl['detailUrl'] = detail_url
                    else:
                        print(f"âš ï¸  KhÃ´ng crawl Ä‘Æ°á»£c detail")
                    
                    # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block
                    if i < len(all_girls):
                        await asyncio.sleep(2)
                else:
                    print(f"âš ï¸  Girl {i} khÃ´ng cÃ³ detailUrl")
        
        # Save all to one JSON file
        result = self.save_to_json(all_girls)
        
        return {
            "totalCrawled": len(all_girls),
            "totalSaved": result.get("saved", 0),
            "file": result.get("file", "")
        }
    
    async def crawl_all_listing_pages(self, start_page: int = 1, max_pages: int = None):
        """Crawl táº¥t cáº£ cÃ¡c trang listing (cÃ³ thá»ƒ detect sá»‘ trang tá»‘i Ä‘a)
        
        Returns:
            Dict: {"girls": List[Dict], "listing_file": str}
        """
        all_girls = []
        current_page = start_page
        
        print(f"\n{'='*50}")
        print(f"ğŸ“‹ GIAI ÄOáº N 1: Crawl listing pages")
        if max_pages:
            print(f"   Tá»« trang {start_page} Ä‘áº¿n {start_page + max_pages - 1}")
        else:
            print(f"   Tá»« trang {start_page} (tá»± Ä‘á»™ng detect sá»‘ trang)")
        print(f"{'='*50}\n")
        
        while True:
            if max_pages and current_page > start_page + max_pages - 1:
                break
            
            print(f"\nğŸ“„ Trang {current_page}")
            girls = await self.crawl_girls_list(current_page, 60)
            
            if not girls or len(girls) == 0:
                print(f"âš ï¸  Trang {current_page} khÃ´ng cÃ³ dá»¯ liá»‡u, dá»«ng láº¡i")
                break
            
            all_girls.extend(girls)
            print(f"âœ… ÄÃ£ cÃ³ tá»•ng cá»™ng {len(all_girls)} girls\n")
            
            # Delay between pages
            print(f"â³ Äá»£i 5 giÃ¢y trÆ°á»›c khi crawl trang tiáº¿p theo...\n")
            await asyncio.sleep(5)
            
            current_page += 1
        
        # LÆ°u danh sÃ¡ch listing
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        listing_file = f"listing_{timestamp}.json"
        result = self.save_to_json(all_girls, listing_file)
        
        print(f"\nâœ… HoÃ n thÃ nh crawl listing: {len(all_girls)} girls")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u danh sÃ¡ch vÃ o: {result.get('file', '')}\n")
        
        return {
            "girls": all_girls,
            "listing_file": result.get("file", "")
        }
    
    async def _crawl_one_girl_detail(self, girl: Dict, index: int, total: int, save_individual: bool = True, save_combined: bool = False, combined_file: str = None, all_details: list = None) -> tuple:
        """Crawl detail cho 1 gÃ¡i (dÃ¹ng trong concurrent crawling)
        
        Returns:
            (success: bool, girl: Dict)
        """
        async with self.semaphore:  # Giá»›i háº¡n sá»‘ lÆ°á»£ng concurrent
            if not girl.get('detailUrl'):
                return (False, girl)
            
            girl_name = girl.get('name', 'N/A')[:40]
            print(f"[{index}/{total}] ğŸ” Äang crawl: {girl_name}...")
            
            try:
                # Random delay Ä‘á»ƒ trÃ¡nh pattern detection
                delay = random.uniform(self.delay_min, self.delay_max)
                await asyncio.sleep(delay)
                
                detail_data = await self.crawl_girl_detail(girl['detailUrl'])
                if detail_data:
                    # Merge detail data
                    detail_url = girl.get('detailUrl')
                    girl.update(detail_data)
                    girl['detailUrl'] = detail_url
                    
                    # LÆ°u vÃ o file riÃªng náº¿u Ä‘Æ°á»£c yÃªu cáº§u
                    if save_individual:
                        filepath = self.save_girl_detail_to_file(girl)
                        if filepath:
                            print(f"[{index}/{total}] âœ… {girl_name[:30]}... â†’ {os.path.basename(filepath)}")
                    
                    # LÆ°u vÃ o combined file náº¿u Ä‘Æ°á»£c yÃªu cáº§u (incremental)
                    if save_combined and combined_file and all_details is not None:
                        async with self.file_lock:  # Thread-safe
                            all_details.append(girl)
                            # LÆ°u ngay sau má»—i item
                            try:
                                with open(combined_file, 'w', encoding='utf-8') as f:
                                    json.dump(all_details, f, ensure_ascii=False, indent=2)
                            except Exception as e:
                                print(f"âš ï¸  Lá»—i khi lÆ°u combined file: {e}")
                    
                    return (True, girl)
                else:
                    print(f"[{index}/{total}] âš ï¸  KhÃ´ng crawl Ä‘Æ°á»£c: {girl_name[:30]}...")
                    return (False, girl)
            except Exception as e:
                print(f"[{index}/{total}] âŒ Lá»—i: {girl_name[:30]}... - {str(e)}")
                return (False, girl)
    
    async def crawl_details_from_listing_file(self, listing_file: str, save_individual: bool = True, batch_size: int = None, save_combined: bool = False):
        """Äá»c file listing vÃ  crawl detail cho tá»«ng gÃ¡i (concurrent)
        
        Args:
            listing_file: ÄÆ°á»ng dáº«n Ä‘áº¿n file JSON chá»©a danh sÃ¡ch girls
            save_individual: Náº¿u True, lÆ°u má»—i gÃ¡i vÃ o file riÃªng vá»›i tÃªn gÃ¡i
            batch_size: Sá»‘ lÆ°á»£ng girls crawl má»—i batch (None = táº¥t cáº£ cÃ¹ng lÃºc)
            save_combined: Náº¿u True, gá»™m táº¥t cáº£ vÃ o 1 JSON file vÃ  lÆ°u incremental
        """
        # Äá»c file listing
        if not os.path.exists(listing_file):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {listing_file}")
            return None
        
        with open(listing_file, 'r', encoding='utf-8') as f:
            girls = json.load(f)
        
        # Lá»c nhá»¯ng girls cÃ³ detailUrl
        valid_girls = [(i, girl) for i, girl in enumerate(girls, 1) if girl.get('detailUrl')]
        
        print(f"\n{'='*50}")
        print(f"ğŸ” GIAI ÄOáº N 2: Crawl detail cho {len(valid_girls)} girls")
        print(f"   Tá»« file: {listing_file}")
        print(f"   LÆ°u riÃªng tá»«ng file: {save_individual}")
        print(f"   Gá»™m vÃ o 1 JSON: {save_combined}")
        print(f"   Concurrent: {self.max_concurrent} requests")
        print(f"   Delay: {self.delay_min}-{self.delay_max} giÃ¢y")
        if batch_size:
            print(f"   Batch size: {batch_size}")
        print(f"{'='*50}\n")
        
        success_count = 0
        failed_count = 0
        
        # Táº¡o file combined náº¿u cáº§n
        combined_file = None
        all_details = []
        if save_combined:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            combined_file = os.path.join("data", f"all_girls_details_{timestamp}.json")
            os.makedirs("data", exist_ok=True)
            print(f"ğŸ’¾ File gá»™m: {combined_file}\n")
        
        # Crawl theo batch hoáº·c táº¥t cáº£ cÃ¹ng lÃºc
        if batch_size:
            # Crawl theo batch
            for batch_start in range(0, len(valid_girls), batch_size):
                batch_end = min(batch_start + batch_size, len(valid_girls))
                batch = valid_girls[batch_start:batch_end]
                
                print(f"\nğŸ“¦ Batch {batch_start//batch_size + 1}: {len(batch)} girls\n")
                
                # Táº¡o tasks cho batch nÃ y
                tasks = [
                    self._crawl_one_girl_detail(girl, index, len(valid_girls), save_individual, save_combined, combined_file, all_details)
                    for index, girl in batch
                ]
                
                # Chá» táº¥t cáº£ tasks trong batch hoÃ n thÃ nh
                results = await asyncio.gather(*tasks)
                
                # Äáº¿m káº¿t quáº£
                for success, updated_girl in results:
                    if success:
                        success_count += 1
                    else:
                        failed_count += 1
                
                # Delay giá»¯a cÃ¡c batch
                if batch_end < len(valid_girls):
                    batch_delay = random.uniform(5, 10)
                    print(f"\nâ³ Äá»£i {batch_delay:.1f} giÃ¢y trÆ°á»›c batch tiáº¿p theo...\n")
                    await asyncio.sleep(batch_delay)
        else:
            # Crawl táº¥t cáº£ cÃ¹ng lÃºc (giá»›i háº¡n bá»Ÿi semaphore)
            print(f"ğŸš€ Báº¯t Ä‘áº§u crawl {len(valid_girls)} girls (concurrent: {self.max_concurrent})\n")
            
            tasks = [
                self._crawl_one_girl_detail(girl, index, len(valid_girls), save_individual, save_combined, combined_file, all_details)
                for index, girl in valid_girls
            ]
            
            results = await asyncio.gather(*tasks)
            
            # Äáº¿m káº¿t quáº£
            for success, updated_girl in results:
                if success:
                    success_count += 1
                else:
                    failed_count += 1
        
        # LÆ°u táº¥t cáº£ vÃ o 1 file tá»•ng há»£p (náº¿u cáº§n vÃ  chÆ°a lÆ°u incremental)
        if not save_individual and not save_combined:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            all_details_file = f"all_details_{timestamp}.json"
            result = self.save_to_json(girls, all_details_file)
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u táº¥t cáº£ detail vÃ o: {result.get('file', '')}")
        
        # Náº¿u Ä‘Ã£ lÆ°u incremental, chá»‰ thÃ´ng bÃ¡o
        if save_combined and combined_file:
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u táº¥t cáº£ {len(all_details)} girls vÃ o: {combined_file}")
        
        print(f"\n{'='*50}")
        print(f"âœ… HOÃ€N THÃ€NH CRAWL DETAIL")
        print(f"   âœ… ThÃ nh cÃ´ng: {success_count}")
        print(f"   âŒ Tháº¥t báº¡i: {failed_count}")
        if len(valid_girls) > 0:
            print(f"   ğŸ“Š Tá»· lá»‡: {success_count/len(valid_girls)*100:.1f}%")
        print(f"{'='*50}\n")
        
        return {
            "total": len(valid_girls),
            "success": success_count,
            "failed": failed_count
        }


async def main():
    """Main function"""
    import sys
    
    # Parse concurrent settings
    max_concurrent = 3  # Máº·c Ä‘á»‹nh
    delay_min = 2.0
    delay_max = 5.0
    batch_size = None
    
    for i, arg in enumerate(sys.argv):
        if arg == '--concurrent' and i + 1 < len(sys.argv):
            max_concurrent = int(sys.argv[i + 1])
        elif arg == '--delay-min' and i + 1 < len(sys.argv):
            delay_min = float(sys.argv[i + 1])
        elif arg == '--delay-max' and i + 1 < len(sys.argv):
            delay_max = float(sys.argv[i + 1])
        elif arg == '--batch-size' and i + 1 < len(sys.argv):
            batch_size = int(sys.argv[i + 1])
    
    crawler = GirlCrawler(max_concurrent=max_concurrent, delay_min=delay_min, delay_max=delay_max)
    
    try:
        # Parse arguments
        args = [arg for arg in sys.argv[1:] if not arg.startswith('--') and not arg.startswith('-')]
        flags = [arg for arg in sys.argv[1:] if arg.startswith('--') or arg.startswith('-')]
        
        # Parse flags
        crawl_details = '--detail' in sys.argv or '-d' in sys.argv
        listing_only = '--listing-only' in sys.argv or '--all-listing' in sys.argv
        auto_mode = '--auto' in sys.argv or '--full' in sys.argv  # Tá»± Ä‘á»™ng crawl listing + detail
        detail_from_file = None
        for i, arg in enumerate(sys.argv):
            if arg in ['--detail-from-file', '--from-file'] and i + 1 < len(sys.argv):
                detail_from_file = sys.argv[i + 1]
                break
        save_individual = '--save-individual' in sys.argv or '--individual' in sys.argv
        save_combined = '--save-combined' in sys.argv or '--combined' in sys.argv or '--gop' in sys.argv
        auto_mode = '--auto' in sys.argv or '--full' in sys.argv  # Tá»± Ä‘á»™ng crawl listing + detail
        
        # Mode 0: Auto mode - Tá»± Ä‘á»™ng crawl listing + detail
        if auto_mode:
            max_pages = None
            if len(args) > 0:
                try:
                    max_pages = int(args[0])
                except:
                    pass
            
            print(f"\n{'='*60}")
            print(f"ğŸš€ CHáº¾ Äá»˜ Tá»° Äá»˜NG: Crawl listing + detail")
            print(f"{'='*60}")
            if max_pages:
                print(f"   ğŸ“‹ Sá»‘ trang listing tá»‘i Ä‘a: {max_pages}")
            else:
                print(f"   ğŸ“‹ Crawl táº¥t cáº£ trang listing (tá»± Ä‘á»™ng detect)")
            print(f"   ğŸ” Tá»± Ä‘á»™ng crawl detail sau khi xong listing")
            print(f"   ğŸ’¾ LÆ°u riÃªng tá»«ng file: {save_individual}")
            print(f"   ğŸ”„ Concurrent: {max_concurrent}")
            print(f"   â±ï¸  Delay: {delay_min}-{delay_max}s")
            if batch_size:
                print(f"   ğŸ“¦ Batch size: {batch_size}")
            print(f"{'='*60}\n")
            
            # Giai Ä‘oáº¡n 1: Crawl listing
            listing_result = await crawler.crawl_all_listing_pages(1, max_pages)
            listing_file = listing_result.get('listing_file', '')
            
            if not listing_file or not os.path.exists(listing_file):
                print("âŒ KhÃ´ng tÃ¬m tháº¥y file listing, dá»«ng láº¡i")
                return
            
            print(f"\n{'='*60}")
            print(f"âœ… HoÃ n thÃ nh crawl listing: {len(listing_result.get('girls', []))} girls")
            print(f"ğŸ’¾ File: {listing_file}")
            print(f"{'='*60}\n")
            
            # Há»i xÃ¡c nháº­n (cÃ³ thá»ƒ bá» qua náº¿u muá»‘n)
            print("â³ Báº¯t Ä‘áº§u crawl detail sau 3 giÃ¢y...\n")
            await asyncio.sleep(3)
            
            # Giai Ä‘oáº¡n 2: Crawl detail
            detail_result = await crawler.crawl_details_from_listing_file(
                listing_file, 
                save_individual, 
                batch_size,
                save_combined
            )
            
            print(f"\n{'='*60}")
            print("âœ… HOÃ€N THÃ€NH Táº¤T Cáº¢!")
            print(f"{'='*60}")
            print(f"ğŸ“Š Listing: {len(listing_result.get('girls', []))} girls")
            if detail_result:
                print(f"ğŸ“Š Detail - ThÃ nh cÃ´ng: {detail_result.get('success', 0)}")
                print(f"ğŸ“Š Detail - Tháº¥t báº¡i: {detail_result.get('failed', 0)}")
            print(f"ğŸ’¾ File listing: {listing_file}")
            if save_individual:
                print(f"ğŸ’¾ Files detail: data/details/*.json")
            print(f"{'='*60}\n")
            return
        
        # Mode 1: Crawl detail tá»« file listing
        if detail_from_file:
            print(f"ğŸš€ CHIáº¾N LÆ¯á»¢C: Crawl detail tá»« file listing")
            print(f"   ğŸ“ File: {detail_from_file}")
            print(f"   ğŸ’¾ LÆ°u riÃªng tá»«ng file: {save_individual}")
            print(f"   ğŸ’¾ Gá»™m vÃ o 1 JSON: {save_combined}")
            print(f"   ğŸ”„ Concurrent: {max_concurrent}")
            print(f"   â±ï¸  Delay: {delay_min}-{delay_max}s")
            if batch_size:
                print(f"   ğŸ“¦ Batch size: {batch_size}")
            print()
            result = await crawler.crawl_details_from_listing_file(detail_from_file, save_individual, batch_size, save_combined)
            print(f"\n{'='*50}")
            print("âœ… HOÃ€N THÃ€NH!")
            print(f"{'='*50}")
            if result:
                print(f"ğŸ“Š Tá»•ng: {result.get('total', 0)}")
                print(f"âœ… ThÃ nh cÃ´ng: {result.get('success', 0)}")
                print(f"âŒ Tháº¥t báº¡i: {result.get('failed', 0)}")
            return
        
        # Mode 2: Chá»‰ crawl listing (táº¥t cáº£ trang)
        if listing_only:
            max_pages = None
            if len(args) > 0:
                max_pages = int(args[0])
            
            print(f"ğŸš€ CHIáº¾N LÆ¯á»¢C: Crawl listing only")
            if max_pages:
                print(f"   ğŸ“‹ Sá»‘ trang tá»‘i Ä‘a: {max_pages}")
            else:
                print(f"   ğŸ“‹ Crawl táº¥t cáº£ trang (tá»± Ä‘á»™ng detect)")
            print()
            
            result = await crawler.crawl_all_listing_pages(1, max_pages)
            print(f"\n{'='*50}")
            print("âœ… HOÃ€N THÃ€NH!")
            print(f"{'='*50}")
            print(f"ğŸ“Š Tá»•ng crawl: {len(result.get('girls', []))} girls")
            print(f"ğŸ’¾ File listing: {result.get('listing_file', 'N/A')}")
            print(f"\nğŸ’¡ Äá»ƒ crawl detail, cháº¡y:")
            print(f"   python main.py --detail-from-file \"{result.get('listing_file', '')}\" --save-individual")
            return
        
        # Mode 3: Crawl bÃ¬nh thÆ°á»ng (cÃ³ thá»ƒ kÃ¨m detail)
        if len(args) > 0:
            if len(args) >= 2:
                start_page = int(args[0])
                end_page = int(args[1])
                print(f"ğŸš€ CHIáº¾N LÆ¯á»¢C CRAWL:")
                print(f"   ğŸ“‹ Listing pages: {start_page} Ä‘áº¿n {end_page}")
                print(f"   ğŸ” Crawl detail: {crawl_details}")
                if crawl_details and save_individual:
                    print(f"   ğŸ’¾ LÆ°u riÃªng tá»«ng file: {save_individual}")
                print()
                
                if crawl_details and save_individual:
                    # Crawl listing trÆ°á»›c
                    all_girls = []
                    for page_num in range(start_page, end_page + 1):
                        print(f"\nğŸ“„ Trang {page_num}/{end_page}")
                        girls = await crawler.crawl_girls_list(page_num, 60)
                        all_girls.extend(girls)
                        print(f"âœ… ÄÃ£ cÃ³ tá»•ng cá»™ng {len(all_girls)} girls\n")
                        if page_num < end_page:
                            await asyncio.sleep(5)
                    
                    # Crawl detail vÃ  lÆ°u riÃªng
                    print(f"\n{'='*50}")
                    print(f"ğŸ” Crawl detail cho {len(all_girls)} girls")
                    print(f"{'='*50}\n")
                    
                    for i, girl in enumerate(all_girls, 1):
                        if girl.get('detailUrl'):
                            print(f"[{i}/{len(all_girls)}] {girl.get('name', 'N/A')[:40]}...")
                            detail_data = await crawler.crawl_girl_detail(girl['detailUrl'])
                            if detail_data:
                                detail_url = girl.get('detailUrl')
                                girl.update(detail_data)
                                girl['detailUrl'] = detail_url
                                filepath = crawler.save_girl_detail_to_file(girl)
                                if filepath:
                                    print(f"   âœ… {os.path.basename(filepath)}")
                            await asyncio.sleep(2)
                    
                    result = {"totalCrawled": len(all_girls)}
                else:
                    result = await crawler.crawl_multiple_pages(start_page, end_page, crawl_details)
            else:
                page = int(args[0])
                limit = int(args[1]) if len(args) > 1 else 60
                print(f"ğŸš€ CHIáº¾N LÆ¯á»¢C CRAWL:")
                print(f"   ğŸ“‹ Listing page: {page} (limit: {limit})")
                print(f"   ğŸ” Crawl detail: {crawl_details}\n")
                result = await crawler.crawl_and_save(page, limit, crawl_details)
        else:
            # Default: crawl page 1
            print(f"ğŸš€ CHIáº¾N LÆ¯á»¢C CRAWL:")
            print(f"   ğŸ“‹ Listing page: 1 (máº·c Ä‘á»‹nh)")
            print(f"   ğŸ” Crawl detail: {crawl_details}\n")
            result = await crawler.crawl_and_save(1, 60, crawl_details)
        
        print(f"\n{'='*50}")
        print("âœ… HOÃ€N THÃ€NH!")
        print(f"{'='*50}")
        print(f"ğŸ“Š Tá»•ng crawl: {result.get('totalCrawled', result.get('crawled', 0))} girls")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o: {result.get('file', 'N/A')}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        import traceback
        print(f"\nâŒ Lá»—i: {str(e)}")
        traceback.print_exc()
    finally:
        try:
            await crawler.close_browser()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi Ä‘Ã³ng browser: {e}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"âŒ Lá»—i chÆ°Æ¡ng trÃ¬nh: {e}")

