"""
Crawler Ä‘á»ƒ láº¥y thÃ´ng tin chi tiáº¿t cá»§a gÃ¡i chat sex tá»« gaigu1.net/chat-sex.
Sá»­ dá»¥ng Playwright Ä‘á»ƒ táº£i trang vÃ  trÃ­ch xuáº¥t thÃ´ng tin.
"""

import asyncio
import json
import os
import random
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

from playwright.async_api import async_playwright, Browser


class ChatSexDetailCrawler:
    def __init__(
        self,
        delay_min: float = 0.3,
        delay_max: float = 0.8,
        headless: bool = True,
    ):
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.headless = headless

    async def init_browser(self):
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless, args=["--no-sandbox", "--disable-setuid-sandbox"]
            )
        return self.browser

    async def close_browser(self):
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi Ä‘Ã³ng browser: {e}")

    async def crawl_chat_sex_detail(self, url: str) -> Dict:
        """
        Crawl thÃ´ng tin chi tiáº¿t cá»§a gÃ¡i chat sex.
        
        Args:
            url: URL cá»§a trang gÃ¡i chat sex cáº§n crawl
            
        Returns:
            Dict chá»©a thÃ´ng tin chi tiáº¿t
        """
        await self.init_browser()
        page = await self.browser.new_page()
        await page.set_extra_http_headers(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://gaigu1.net/",
            }
        )

        try:
            print(f"ğŸ” Äang crawl: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=20000)
            await asyncio.sleep(random.uniform(self.delay_min, self.delay_max))

            # Láº¥y thÃ´ng tin chi tiáº¿t tá»« container XPath: /html/body/div[5]/div[4]/div[3]
            detail_info = await page.evaluate(
                """
                () => {
                    const data = {
                        url: window.location.href,
                        title: '',
                        name: '',
                        age: null,
                        location: '',
                        price: '',
                        description: '',
                        images: [],
                        phone: '',
                        zalo: '',
                        telegram: '',
                        services: [],
                        workingHours: '',
                        verified: false,
                        rating: null,
                        viewCount: 0,
                        tags: []
                    };

                    // Sá»­ dá»¥ng XPath Ä‘á»ƒ tÃ¬m container detail
                    // XPath: /html/body/div[5]/div[4]/div[3]
                    const xpath = '/html/body/div[5]/div[4]/div[3]';
                    const result = document.evaluate(
                        xpath,
                        document,
                        null,
                        XPathResult.FIRST_ORDERED_NODE_TYPE,
                        null
                    );
                    const detailContainer = result.singleNodeValue;

                    // Náº¿u khÃ´ng tÃ¬m tháº¥y container, fallback vá» cÃ¡ch cÅ©
                    const container = detailContainer || document.body;

                    // Láº¥y title
                    data.title = container.querySelector('h1')?.textContent?.trim() || 
                                container.querySelector('.title')?.textContent?.trim() || 
                                document.querySelector('title')?.textContent?.trim() || '';

                    // Láº¥y tÃªn (cÃ³ thá»ƒ tá»« title hoáº·c element riÃªng)
                    data.name = container.querySelector('.name')?.textContent?.trim() ||
                               container.querySelector('[class*="name"]')?.textContent?.trim() ||
                               data.title.split('-')[0]?.trim() ||
                               data.title;

                    // Láº¥y tuá»•i
                    const ageText = document.body.innerText.match(/(?:tuá»•i|age)[\s:]*(\d+)/i);
                    if (ageText) {
                        data.age = parseInt(ageText[1]);
                    }

                    // Láº¥y Ä‘á»‹a Ä‘iá»ƒm (tá»« container)
                    data.location = container.querySelector('.location')?.textContent?.trim() ||
                                  container.querySelector('[class*="location"]')?.textContent?.trim() ||
                                  container.querySelector('[class*="address"]')?.textContent?.trim() ||
                                  '';

                    // Láº¥y giÃ¡ (tá»« container)
                    const containerText = container.innerText || container.textContent || '';
                    const priceText = containerText.match(/(?:giÃ¡|price)[\s:]*([\d.,]+)/i);
                    if (priceText) {
                        data.price = priceText[1];
                    }

                    // Láº¥y mÃ´ táº£ (tá»« container)
                    data.description = container.querySelector('.description')?.textContent?.trim() ||
                                     container.querySelector('[class*="description"]')?.textContent?.trim() ||
                                     container.querySelector('.content')?.textContent?.trim() ||
                                     '';

                    // Láº¥y áº£nh (tá»« container)
                    const imageSelectors = [
                        'img[src*="chat"]',
                        'img[data-src*="chat"]',
                        '.gallery img',
                        '.images img',
                        '[class*="image"] img',
                        '[class*="photo"] img',
                        'img[src*="gaigu1.net"]',
                        'img'
                    ];
                    
                    const imageSet = new Set();
                    imageSelectors.forEach(selector => {
                        container.querySelectorAll(selector).forEach(img => {
                            let src = img.getAttribute('src') || img.getAttribute('data-src') || img.getAttribute('data-lazy-src');
                            if (src) {
                                if (src.startsWith('//')) {
                                    src = 'https:' + src;
                                } else if (src.startsWith('/')) {
                                    src = window.location.origin + src;
                                }
                                if (src.includes('gaigu1.net') && !src.includes('logo') && !src.includes('icon') && !src.includes('avatar')) {
                                    imageSet.add(src);
                                }
                            }
                        });
                    });
                    data.images = Array.from(imageSet);

                    // Láº¥y sá»‘ Ä‘iá»‡n thoáº¡i (tá»« container)
                    const phoneMatch = containerText.match(/(?:0|\+84)[\d\s\-]{9,11}/);
                    if (phoneMatch) {
                        data.phone = phoneMatch[0].replace(/[\s\-]/g, '');
                    }

                    // Láº¥y Zalo (tá»« container)
                    const zaloMatch = containerText.match(/zalo[:\s]*([\d\w]+)/i);
                    if (zaloMatch) {
                        data.zalo = zaloMatch[1];
                    }

                    // Láº¥y Telegram (tá»« container)
                    const telegramMatch = containerText.match(/telegram[:\s]*([\d\w@]+)/i);
                    if (telegramMatch) {
                        data.telegram = telegramMatch[1];
                    }

                    // Láº¥y dá»‹ch vá»¥ (tá»« container)
                    const serviceElements = container.querySelectorAll('[class*="service"], [class*="dich-vu"]');
                    serviceElements.forEach(el => {
                        const serviceText = el.textContent?.trim();
                        if (serviceText) {
                            data.services.push(serviceText);
                        }
                    });

                    // Láº¥y giá» lÃ m viá»‡c (tá»« container)
                    const hoursMatch = containerText.match(/(?:giá»|hours?)[\s:]*([\d\s\-:]+)/i);
                    if (hoursMatch) {
                        data.workingHours = hoursMatch[1];
                    }

                    // Kiá»ƒm tra verified (tá»« container)
                    data.verified = !!container.querySelector('[class*="verified"], [class*="check"]');

                    // Láº¥y rating (tá»« container)
                    const ratingEl = container.querySelector('[class*="rating"], [class*="star"]');
                    if (ratingEl) {
                        const ratingText = ratingEl.textContent?.match(/([\d.]+)/);
                        if (ratingText) {
                            data.rating = parseFloat(ratingText[1]);
                        }
                    }

                    // Láº¥y view count (tá»« container)
                    const viewMatch = containerText.match(/(?:lÆ°á»£t xem|views?)[\s:]*(\d+)/i);
                    if (viewMatch) {
                        data.viewCount = parseInt(viewMatch[1]);
                    }

                    // Láº¥y tags (tá»« container)
                    const tagElements = container.querySelectorAll('[class*="tag"], .tags a, [class*="label"]');
                    tagElements.forEach(tag => {
                        const tagText = tag.textContent?.trim();
                        if (tagText) {
                            data.tags.push(tagText);
                        }
                    });

                    return data;
                }
                """
            )

            detail_info['crawled_at'] = datetime.now().isoformat()
            detail_info['total_images'] = len(detail_info.get('images', []))

            print(f"   âœ“ TÃ¬m tháº¥y: {detail_info.get('name', 'N/A')} - {len(detail_info.get('images', []))} áº£nh")
            
            return detail_info

        except Exception as e:
            print(f"âŒ Lá»—i khi crawl {url}: {e}")
            return {
                'url': url,
                'error': str(e),
                'crawled_at': datetime.now().isoformat()
            }
        finally:
            await page.close()

    async def crawl_multiple(self, urls: List[str], output_dir: str = "data/chat_sex_details", batch_size: int = 5) -> List[Dict]:
        """
        Crawl nhiá»u URL vá»›i concurrent requests Ä‘á»ƒ tÄƒng tá»‘c.
        
        Args:
            urls: List cÃ¡c URL cáº§n crawl
            output_dir: ThÆ° má»¥c Ä‘á»ƒ lÆ°u file JSON
            batch_size: Sá»‘ request Ä‘á»“ng thá»i
            
        Returns:
            List cÃ¡c dict chá»©a thÃ´ng tin chi tiáº¿t
        """
        os.makedirs(output_dir, exist_ok=True)
        results = []
        
        # Crawl theo batch Ä‘á»ƒ tÄƒng tá»‘c
        for batch_start in range(0, len(urls), batch_size):
            batch_end = min(batch_start + batch_size, len(urls))
            batch_urls = urls[batch_start:batch_end]
            
            print(f"[{batch_start + 1}-{batch_end}/{len(urls)}] Äang crawl batch {batch_start // batch_size + 1}...")
            
            # Crawl Ä‘á»“ng thá»i trong batch
            batch_tasks = [self.crawl_chat_sex_detail(url) for url in batch_urls]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            for idx, (url, detail) in enumerate(zip(batch_urls, batch_results)):
                if isinstance(detail, Exception):
                    print(f"   âŒ Lá»—i {url}: {detail}")
                    detail = {'url': url, 'error': str(detail), 'crawled_at': datetime.now().isoformat()}
                
                results.append(detail)
                
                # LÆ°u tá»«ng file riÃªng
                if 'error' not in detail:
                    filename = url.split('/')[-1] or url.split('/')[-2]
                    filename = filename.replace('/', '_').replace('?', '_')
                    output_file = os.path.join(output_dir, f"chat_sex_{filename}.json")
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(detail, f, ensure_ascii=False, indent=2)
            
            # Delay ngáº¯n giá»¯a cÃ¡c batch
            if batch_end < len(urls):
                await asyncio.sleep(0.5)
        
        return results


async def main():
    """Main function Ä‘á»ƒ cháº¡y crawler."""
    # Äá»c danh sÃ¡ch links tá»« file JSON
    import glob
    
    # TÃ¬m file links má»›i nháº¥t
    link_files = glob.glob("data/chat_sex_links_*.json")
    if not link_files:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file links. HÃ£y cháº¡y chat_sex_listing_crawler.py trÆ°á»›c!")
        return
    
    latest_link_file = max(link_files, key=os.path.getctime)
    print(f"ğŸ“‚ Äá»c links tá»«: {latest_link_file}")
    
    with open(latest_link_file, "r", encoding="utf-8") as f:
        urls = json.load(f)
    
    print(f"ğŸ“Š Tá»•ng cá»™ng: {len(urls)} URL cáº§n crawl")
    
    crawler = ChatSexDetailCrawler(
        delay_min=0.3,
        delay_max=0.8,
        headless=True  # KhÃ´ng má»Ÿ Chrome
    )

    try:
        print("=" * 60)
        print("ğŸš€ Báº®T Äáº¦U CRAWL CHAT SEX DETAILS")
        print("=" * 60)
        
        # Crawl táº¥t cáº£
        results = await crawler.crawl_multiple(urls)
        
        # LÆ°u tá»•ng há»£p
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/chat_sex_details_all_{timestamp}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print("=" * 60)
        print(f"âœ… HOÃ€N Táº¤T! ÄÃ£ crawl {len(results)} gÃ¡i chat sex")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_file}")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\nâš ï¸  ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await crawler.close_browser()


if __name__ == "__main__":
    asyncio.run(main())

