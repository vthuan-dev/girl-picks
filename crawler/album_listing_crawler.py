"""
Crawler Ä‘á»ƒ láº¥y táº¥t cáº£ link album tá»« trang listing cá»§a gaigu1.net/anh-sex.
Sá»­ dá»¥ng Playwright Ä‘á»ƒ crawl nhiá»u trang vÃ  trÃ­ch xuáº¥t táº¥t cáº£ link album.
"""

import asyncio
import json
import os
import random
import re
from datetime import datetime
from typing import List, Optional, Set
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse

from playwright.async_api import async_playwright, Browser


class AlbumListingCrawler:
    def __init__(
        self,
        delay_min: float = 1.0,
        delay_max: float = 2.5,
        headless: bool = False,
    ):
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.headless = headless

    async def init_browser(self):
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless, args=["--no-sandbox", "--disable-setuid-sandbox"]
            )
        return self.browser

    async def close_browser(self):
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi Ä‘Ã³ng browser: {e}")

    async def extract_album_links_from_page(self, page, base_url: str) -> Set[str]:
        """
        TrÃ­ch xuáº¥t táº¥t cáº£ link album tá»« trang hiá»‡n táº¡i.
        
        Returns:
            Set cÃ¡c URL album
        """
        album_links = await page.evaluate(
            """
            (baseUrl) => {
                const links = new Set();
                const baseUrlObj = new URL(baseUrl);
                
                // TÃ¬m táº¥t cáº£ link cÃ³ chá»©a /album-anh-sex/
                document.querySelectorAll('a[href*="/album-anh-sex/"]').forEach(link => {
                    const href = link.getAttribute('href');
                    if (href) {
                        // Chuáº©n hÃ³a URL
                        let fullUrl;
                        if (href.startsWith('http')) {
                            fullUrl = href;
                        } else {
                            fullUrl = new URL(href, baseUrl).href;
                        }
                        
                        // Chá»‰ láº¥y link album (cÃ³ format /album-anh-sex/ID/title)
                        if (fullUrl.includes('/album-anh-sex/') && /\/album-anh-sex\/\d+\//.test(fullUrl)) {
                            // Loáº¡i bá» fragment vÃ  query params khÃ´ng cáº§n thiáº¿t
                            const urlObj = new URL(fullUrl);
                            urlObj.hash = '';
                            // Loáº¡i bá» query params Ä‘á»ƒ cÃ³ URL sáº¡ch
                            urlObj.search = '';
                            links.add(urlObj.href);
                        }
                    }
                });
                
                return Array.from(links);
            }
            """,
            base_url
        )
        
        return set(album_links)

    async def get_total_pages(self, page) -> int:
        """TÃ¬m tá»•ng sá»‘ trang cÃ³ thá»ƒ crawl."""
        try:
            # Thá»­ nhiá»u cÃ¡ch Ä‘á»ƒ tÃ¬m pagination
            total_pages = await page.evaluate(
                """
                () => {
                    // CÃ¡ch 1: TÃ¬m trong pagination
                    const pagination = document.querySelector('.pagination') || 
                                      document.querySelector('[class*="pagination"]') ||
                                      document.querySelector('.page-numbers');
                    
                    if (pagination) {
                        const pageLinks = pagination.querySelectorAll('a, span');
                        let maxPage = 1;
                        pageLinks.forEach(el => {
                            const text = el.textContent?.trim() || '';
                            const num = parseInt(text);
                            if (!isNaN(num) && num > maxPage) {
                                maxPage = num;
                            }
                        });
                        return maxPage;
                    }
                    
                    // CÃ¡ch 2: TÃ¬m "Trang X / Y" hoáº·c "Page X of Y"
                    const pageInfo = document.body.textContent || '';
                    const match = pageInfo.match(/(?:trang|page)\s+\d+\s*[\/\-]\s*(\d+)/i);
                    if (match) {
                        return parseInt(match[1]);
                    }
                    
                    return 1;
                }
                """
            )
            return max(1, total_pages)
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y pagination: {e}")
            return 1

    async def crawl_listing_page(self, url: str, page_num: Optional[int] = None) -> Set[str]:
        """
        Crawl má»™t trang listing Ä‘á»ƒ láº¥y cÃ¡c link album.
        
        Args:
            url: URL trang listing (cÃ³ thá»ƒ cÃ³ hoáº·c khÃ´ng cÃ³ page parameter)
            page_num: Sá»‘ trang (náº¿u None thÃ¬ crawl trang hiá»‡n táº¡i)
            
        Returns:
            Set cÃ¡c URL album
        """
        await self.init_browser()
        page = await self.browser.new_page()
        
        await page.set_extra_http_headers(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://gaigu1.net/",
            }
        )

        try:
            # XÃ¢y dá»±ng URL vá»›i page number náº¿u cÃ³
            if page_num and page_num > 1:
                # Format má»›i: luÃ´n dÃ¹ng ?page=2
                parsed = urlparse(url)
                query_params = parse_qs(parsed.query)
                query_params['page'] = [str(page_num)]
                new_query = urlencode(query_params, doseq=True)
                page_url = urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, new_query, parsed.fragment
                ))
            else:
                page_url = url

            print(f"ğŸ” Äang crawl trang: {page_url}")
            await page.goto(page_url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(2)  # Äá»£i trang load Ä‘áº§y Ä‘á»§

            # Scroll Ä‘á»ƒ load lazy content
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
            await page.wait_for_timeout(1500)

            # TrÃ­ch xuáº¥t link album
            album_links = await self.extract_album_links_from_page(page, page_url)
            
            print(f"  âœ… TÃ¬m tháº¥y {len(album_links)} album link(s)")
            return album_links

        except Exception as e:
            print(f"âŒ Lá»—i crawl trang {page_url}: {e}")
            return set()
        finally:
            await page.close()

    async def crawl_all_albums(
        self, 
        base_url: str, 
        max_pages: Optional[int] = None,
        start_page: int = 1
    ) -> List[str]:
        """
        Crawl táº¥t cáº£ album tá»« trang listing, cÃ³ thá»ƒ crawl nhiá»u trang.
        
        Args:
            base_url: URL trang listing (vÃ­ dá»¥: https://gaigu1.net/anh-sex)
            max_pages: Sá»‘ trang tá»‘i Ä‘a Ä‘á»ƒ crawl (None = crawl táº¥t cáº£)
            start_page: Trang báº¯t Ä‘áº§u crawl
            
        Returns:
            Danh sÃ¡ch táº¥t cáº£ URL album (Ä‘Ã£ loáº¡i bá» duplicate)
        """
        await self.init_browser()
        page = await self.browser.new_page()
        
        await page.set_extra_http_headers(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://gaigu1.net/",
            }
        )

        all_links: Set[str] = set()
        
        try:
            # Crawl trang Ä‘áº§u tiÃªn Ä‘á»ƒ láº¥y thÃ´ng tin pagination
            print(f"ğŸ” Äang crawl trang Ä‘áº§u tiÃªn: {base_url}")
            await page.goto(base_url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(2)
            
            # Láº¥y link tá»« trang Ä‘áº§u
            first_page_links = await self.extract_album_links_from_page(page, base_url)
            all_links.update(first_page_links)
            print(f"  âœ… Trang 1: {len(first_page_links)} album(s), tá»•ng: {len(all_links)}")

            # TÃ¬m tá»•ng sá»‘ trang
            total_pages = await self.get_total_pages(page)
            if max_pages:
                total_pages = min(total_pages, max_pages)
            
            print(f"ğŸ“„ Tá»•ng sá»‘ trang cáº§n crawl: {total_pages}")

            # Crawl cÃ¡c trang tiáº¿p theo
            for page_num in range(start_page + 1, total_pages + 1):
                await page.close()  # ÄÃ³ng page cÅ©
                page = await self.browser.new_page()  # Táº¡o page má»›i
                await page.set_extra_http_headers(
                    {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                        "Referer": base_url,
                    }
                )

                # XÃ¢y dá»±ng URL trang - Format má»›i: luÃ´n dÃ¹ng ?page=2
                parsed = urlparse(base_url)
                query_params = parse_qs(parsed.query)
                query_params['page'] = [str(page_num)]
                new_query = urlencode(query_params, doseq=True)
                page_url = urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, new_query, parsed.fragment
                ))

                try:
                    print(f"ğŸ” Äang crawl trang {page_num}/{total_pages}: {page_url}")
                    await page.goto(page_url, wait_until="networkidle", timeout=60000)
                    await asyncio.sleep(2)
                    
                    # Scroll Ä‘á»ƒ load lazy content
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                    await page.wait_for_timeout(1500)

                    # TrÃ­ch xuáº¥t link
                    page_links = await self.extract_album_links_from_page(page, page_url)
                    before_count = len(all_links)
                    all_links.update(page_links)
                    new_count = len(all_links) - before_count
                    
                    print(f"  âœ… Trang {page_num}: {len(page_links)} album(s), má»›i: {new_count}, tá»•ng: {len(all_links)}")

                    # Náº¿u khÃ´ng cÃ³ link má»›i nÃ o thÃ¬ cÃ³ thá»ƒ Ä‘Ã£ háº¿t
                    if new_count == 0 and page_num > start_page + 2:
                        print(f"âš ï¸  KhÃ´ng cÃ³ link má»›i á»Ÿ trang {page_num}, cÃ³ thá»ƒ Ä‘Ã£ háº¿t. Dá»«ng crawl.")
                        break

                    # Delay giá»¯a cÃ¡c trang
                    if page_num < total_pages:
                        delay = random.uniform(self.delay_min, self.delay_max)
                        await asyncio.sleep(delay)

                except Exception as e:
                    print(f"âŒ Lá»—i crawl trang {page_num}: {e}")
                    continue

        finally:
            await page.close()

        # Chuyá»ƒn tá»« Set sang List vÃ  sáº¯p xáº¿p
        result = sorted(list(all_links))
        print(f"\nâœ… HoÃ n thÃ nh! Tá»•ng cá»™ng: {len(result)} album link(s)")
        return result

    def save_to_file(self, links: List[str], filename: Optional[str] = None, format: str = "txt") -> str:
        """
        LÆ°u danh sÃ¡ch link vÃ o file.
        
        Args:
            links: Danh sÃ¡ch URL
            filename: TÃªn file (None = tá»± Ä‘á»™ng táº¡o)
            format: "txt" hoáº·c "json"
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"album_links_{timestamp}.{format}"

        data_dir = os.path.join(os.path.dirname(__file__), "data")
        os.makedirs(data_dir, exist_ok=True)
        filepath = os.path.join(data_dir, filename)

        if format == "json":
            data = {
                "urls": links,
                "total": len(links),
                "crawled_at": datetime.now().isoformat(),
            }
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        else:
            # LÆ°u dáº¡ng txt (má»—i dÃ²ng 1 URL)
            with open(filepath, "w", encoding="utf-8") as f:
                for link in links:
                    f.write(f"{link}\n")

        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(links)} link vÃ o: {filepath}")
        return filepath


async def main():
    import sys

    base_url = "https://gaigu1.net/anh-sex"
    max_pages = 32  # Máº·c Ä‘á»‹nh chá»‰ crawl 32 trang
    start_page = 1

    if len(sys.argv) > 1:
        base_url = sys.argv[1]
    if len(sys.argv) > 2:
        try:
            max_pages = int(sys.argv[2])
        except ValueError:
            pass
    if len(sys.argv) > 3:
        try:
            start_page = int(sys.argv[3])
        except ValueError:
            pass

    crawler = AlbumListingCrawler(headless=False, delay_min=1.5, delay_max=3.0)
    try:
        print(f"\nğŸš€ Báº¯t Ä‘áº§u crawl album links tá»«: {base_url}")
        if max_pages:
            print(f"ğŸ“„ Sá»‘ trang tá»‘i Ä‘a: {max_pages}")
        print(f"{'='*60}\n")

        links = await crawler.crawl_all_albums(
            base_url=base_url,
            max_pages=max_pages,
            start_page=start_page
        )

        # LÆ°u vÃ o cáº£ 2 format
        txt_file = crawler.save_to_file(links, format="txt")
        json_file = crawler.save_to_file(links, format="json")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Káº¾T QUáº¢")
        print(f"{'='*60}")
        print(f"âœ… Tá»•ng sá»‘ album link: {len(links)}")
        print(f"ğŸ“ File TXT: {txt_file}")
        print(f"ğŸ“ File JSON: {json_file}")
        
        if links:
            print(f"\nğŸ“‹ Má»™t sá»‘ link Ä‘áº§u tiÃªn:")
            for i, link in enumerate(links[:5], 1):
                print(f"  {i}. {link}")

    finally:
        await crawler.close_browser()


if __name__ == "__main__":
    asyncio.run(main())

