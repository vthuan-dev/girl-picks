"""
Crawler Ä‘á»ƒ láº¥y táº¥t cáº£ link gÃ¡i chat sex tá»« trang listing cá»§a gaigu1.net/chat-sex.
Sá»­ dá»¥ng Playwright Ä‘á»ƒ crawl nhiá»u trang vÃ  trÃ­ch xuáº¥t táº¥t cáº£ link.
"""

import asyncio
import json
import os
import random
import re
from datetime import datetime
from typing import List, Optional, Set
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse

from playwright.async_api import async_playwright, Browser


class ChatSexListingCrawler:
    def __init__(
        self,
        delay_min: float = 1.0,
        delay_max: float = 2.5,
        headless: bool = False,
    ):
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.headless = headless

    async def init_browser(self):
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless, args=["--no-sandbox", "--disable-setuid-sandbox"]
            )
        return self.browser

    async def close_browser(self):
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi Ä‘Ã³ng browser: {e}")

    async def extract_chat_sex_links_from_page(self, page, base_url: str) -> Set[str]:
        """
        TrÃ­ch xuáº¥t táº¥t cáº£ link gÃ¡i chat sex tá»« trang hiá»‡n táº¡i.
        Sá»­ dá»¥ng XPath: /html/body/div[5]/div[1]/div[3]/div[1]
        
        Returns:
            Set cÃ¡c URL gÃ¡i chat sex
        """
        chat_links = await page.evaluate(
            """
            (baseUrl) => {
                const links = new Set();
                
                // Sá»­ dá»¥ng XPath Ä‘á»ƒ tÃ¬m container listing
                // XPath: /html/body/div[5]/div[1]/div[3]/div[1]
                const xpath = '/html/body/div[5]/div[1]/div[3]/div[1]';
                const result = document.evaluate(
                    xpath,
                    document,
                    null,
                    XPathResult.FIRST_ORDERED_NODE_TYPE,
                    null
                );
                const container = result.singleNodeValue;
                
                if (!container) {
                    console.warn('KhÃ´ng tÃ¬m tháº¥y container listing vá»›i XPath:', xpath);
                    // Fallback: tÃ¬m táº¥t cáº£ link cÃ³ chá»©a /chat-sex/
                    document.querySelectorAll('a[href*="/chat-sex/"]').forEach(link => {
                        const href = link.getAttribute('href');
                        if (href) {
                            let fullUrl = href.startsWith('http') ? href : new URL(href, baseUrl).href;
                            if (fullUrl.includes('/chat-sex/') && /\/chat-sex\/\d+/.test(fullUrl)) {
                                const urlObj = new URL(fullUrl);
                                urlObj.hash = '';
                                urlObj.search = '';
                                links.add(urlObj.href);
                            }
                        }
                    });
                    return Array.from(links);
                }
                
                // TÃ¬m táº¥t cáº£ link trong container
                container.querySelectorAll('a[href*="/chat-sex/"]').forEach(link => {
                    const href = link.getAttribute('href');
                    if (href) {
                        // Chuáº©n hÃ³a URL
                        let fullUrl;
                        if (href.startsWith('http')) {
                            fullUrl = href;
                        } else {
                            fullUrl = new URL(href, baseUrl).href;
                        }
                        
                        // Chá»‰ láº¥y link chat sex (cÃ³ format /chat-sex/ID/title hoáº·c /chat-sex/ID)
                        if (fullUrl.includes('/chat-sex/') && /\/chat-sex\/\d+/.test(fullUrl)) {
                            // Loáº¡i bá» fragment vÃ  query params khÃ´ng cáº§n thiáº¿t
                            const urlObj = new URL(fullUrl);
                            urlObj.hash = '';
                            // Loáº¡i bá» query params Ä‘á»ƒ cÃ³ URL sáº¡ch
                            urlObj.search = '';
                            links.add(urlObj.href);
                        }
                    }
                });
                
                return Array.from(links);
            }
            """,
            base_url
        )
        
        return set(chat_links)

    async def get_total_pages(self, page) -> int:
        """TÃ¬m tá»•ng sá»‘ trang cÃ³ thá»ƒ crawl."""
        try:
            total_pages = await page.evaluate(
                """
                () => {
                    // CÃ¡ch 1: TÃ¬m trong pagination
                    const pagination = document.querySelector('.pagination') || 
                                     document.querySelector('.page-numbers') ||
                                     document.querySelector('.paging') ||
                                     document.querySelector('[class*="pagination"]') ||
                                     document.querySelector('[class*="paging"]');
                    
                    if (pagination) {
                        const pageLinks = pagination.querySelectorAll('a, span');
                        let maxPage = 1;
                        pageLinks.forEach(link => {
                            const text = link.textContent.trim();
                            const pageNum = parseInt(text);
                            if (!isNaN(pageNum) && pageNum > maxPage) {
                                maxPage = pageNum;
                            }
                        });
                        if (maxPage > 1) return maxPage;
                    }
                    
                    // CÃ¡ch 2: TÃ¬m "Trang X / Y" hoáº·c "Page X of Y"
                    const pageInfo = document.body.innerText;
                    const match = pageInfo.match(/(?:trang|page)\s+(\d+)\s*(?:\/|of)\s*(\d+)/i);
                    if (match && match[2]) {
                        return parseInt(match[2]);
                    }
                    
                    // CÃ¡ch 3: TÃ¬m "Next" button vÃ  Ä‘áº¿m
                    const nextButtons = document.querySelectorAll('a[href*="page="], a.next, a[class*="next"]');
                    if (nextButtons.length > 0) {
                        // Thá»­ tÃ¬m sá»‘ trang cuá»‘i trong URL
                        let maxPage = 1;
                        nextButtons.forEach(btn => {
                            const href = btn.getAttribute('href');
                            if (href) {
                                const pageMatch = href.match(/page[=_](\d+)/i);
                                if (pageMatch) {
                                    const pageNum = parseInt(pageMatch[1]);
                                    if (pageNum > maxPage) maxPage = pageNum;
                                }
                            }
                        });
                        return maxPage;
                    }
                    
                    return 1;
                }
                """
            )
            return max(1, total_pages)
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh tá»•ng sá»‘ trang: {e}")
            return 1

    async def crawl_listing_page(self, url: str, page_num: int = 1) -> Set[str]:
        """
        Crawl má»™t trang listing cá»¥ thá»ƒ.
        
        Args:
            url: URL base cá»§a trang listing
            page_num: Sá»‘ trang cáº§n crawl
            
        Returns:
            Set cÃ¡c URL gÃ¡i chat sex
        """
        await self.init_browser()
        page = await self.browser.new_page()
        await page.set_extra_http_headers(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://gaigu1.net/",
            }
        )

        try:
            # Táº¡o URL vá»›i page number
            if page_num > 1:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                query_params['page'] = [str(page_num)]
                new_query = urlencode(query_params, doseq=True)
                full_url = urlunparse((
                    parsed_url.scheme,
                    parsed_url.netloc,
                    parsed_url.path,
                    parsed_url.params,
                    new_query,
                    parsed_url.fragment
                ))
            else:
                full_url = url

            print(f"ğŸ“„ Äang crawl trang {page_num}: {full_url}")
            await page.goto(full_url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(random.uniform(self.delay_min, self.delay_max))

            # TrÃ­ch xuáº¥t links
            links = await self.extract_chat_sex_links_from_page(page, full_url)
            print(f"   âœ“ TÃ¬m tháº¥y {len(links)} link gÃ¡i chat sex")

            return links

        except Exception as e:
            print(f"âŒ Lá»—i khi crawl trang {page_num}: {e}")
            return set()
        finally:
            await page.close()

    async def crawl_all_pages(self, base_url: str, max_pages: Optional[int] = None) -> List[str]:
        """
        Crawl táº¥t cáº£ cÃ¡c trang listing.
        
        Args:
            base_url: URL base cá»§a trang listing (vÃ­ dá»¥: https://gaigu1.net/chat-sex)
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl (None = crawl táº¥t cáº£)
            
        Returns:
            List cÃ¡c URL gÃ¡i chat sex (Ä‘Ã£ loáº¡i bá» duplicate)
        """
        await self.init_browser()
        page = await self.browser.new_page()
        await page.set_extra_http_headers(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://gaigu1.net/",
            }
        )

        try:
            print(f"ğŸ” Äang kiá»ƒm tra tá»•ng sá»‘ trang: {base_url}")
            await page.goto(base_url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(1)

            total_pages = await self.get_total_pages(page)
            if max_pages:
                total_pages = min(total_pages, max_pages)
            
            print(f"ğŸ“Š Tá»•ng sá»‘ trang: {total_pages}")
            await page.close()

        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh tá»•ng sá»‘ trang: {e}")
            total_pages = 1
            await page.close()

        # Crawl tá»«ng trang
        all_links = set()
        for page_num in range(1, total_pages + 1):
            links = await self.crawl_listing_page(base_url, page_num)
            all_links.update(links)
            print(f"ğŸ“Š Tá»•ng cá»™ng: {len(all_links)} link (sau trang {page_num})")

        return sorted(list(all_links))


async def main():
    """Main function Ä‘á»ƒ cháº¡y crawler."""
    base_url = "https://gaigu1.net/chat-sex"
    
    crawler = ChatSexListingCrawler(
        delay_min=1.0,
        delay_max=2.5,
        headless=False  # Set True náº¿u khÃ´ng muá»‘n tháº¥y browser
    )

    try:
        print("=" * 60)
        print("ğŸš€ Báº®T Äáº¦U CRAWL CHAT SEX LISTING")
        print("=" * 60)
        
        # Crawl táº¥t cáº£ trang (hoáº·c set max_pages Ä‘á»ƒ test)
        links = await crawler.crawl_all_pages(base_url, max_pages=None)
        
        print("=" * 60)
        print(f"âœ… HOÃ€N Táº¤T! Tá»•ng cá»™ng: {len(links)} link gÃ¡i chat sex")
        print("=" * 60)

        # LÆ°u káº¿t quáº£
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/chat_sex_links_{timestamp}.json"
        
        os.makedirs("data", exist_ok=True)
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(links, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_file}")
        
        # CÅ©ng lÆ°u dáº¡ng text Ä‘á»ƒ dá»… xem
        txt_file = f"data/chat_sex_links_{timestamp}.txt"
        with open(txt_file, "w", encoding="utf-8") as f:
            for link in links:
                f.write(f"{link}\n")
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u text vÃ o: {txt_file}")

    except KeyboardInterrupt:
        print("\nâš ï¸  ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await crawler.close_browser()


if __name__ == "__main__":
    asyncio.run(main())

