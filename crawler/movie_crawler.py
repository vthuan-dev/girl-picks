"""
Crawler ƒë·ªÉ crawl phim sex t·ª´ gaigu1.net/phim-sex
S·ª≠ d·ª•ng Playwright ƒë·ªÉ crawl d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß
"""

import asyncio
import json
import os
import re
import random
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Browser, Page

class MovieCrawler:
    def __init__(self, max_concurrent: int = 3, delay_min: float = 2.0, delay_max: float = 5.0):
        """
        Args:
            max_concurrent: S·ªë l∆∞·ª£ng requests ƒë·ªìng th·ªùi t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: 3)
            delay_min: Delay t·ªëi thi·ªÉu gi·ªØa c√°c requests (gi√¢y)
            delay_max: Delay t·ªëi ƒëa gi·ªØa c√°c requests (gi√¢y)
        """
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.base_url = 'https://gaigu1.net/phim-sex'
        self.max_concurrent = max_concurrent
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.file_lock = asyncio.Lock()  # Lock ƒë·ªÉ ƒë·∫£m b·∫£o thread-safe khi ghi file
        
    async def init_browser(self):
        """Kh·ªüi t·∫°o browser"""
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
        return self.browser
    
    async def close_browser(self):
        """ƒê√≥ng browser"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi ƒë√≥ng browser: {e}")
    
    async def crawl_movies_list(self, page_number: int = 1, limit: int = 60) -> List[Dict]:
        """Crawl danh s√°ch phim t·ª´ trang listing
        
        Args:
            page_number: S·ªë trang (b·∫Øt ƒë·∫ßu t·ª´ 1)
            limit: S·ªë l∆∞·ª£ng phim t·ªëi ƒëa (m·∫∑c ƒë·ªãnh 60)
        """
        await self.init_browser()
        
        url = f"{self.base_url}?page={page_number}" if page_number > 1 else self.base_url
        print(f"üîç ƒêang crawl: {url}")
        
        page = await self.browser.new_page()
        
        try:
            # Set realistic headers
            await page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                'Referer': 'https://gaigu1.net/',
            })
            
            await page.goto(url, wait_until='networkidle', timeout=60000)
            await asyncio.sleep(3)  # Wait for content to load
            
            # Wait for the content container - try multiple selectors
            try:
                await page.wait_for_selector('div.content-row, .row.content-row, [class*="content-row"], .col-6.col-sm-6.col-md-4', timeout=15000)
            except:
                print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y content-row, th·ª≠ ti·∫øp...")
                await asyncio.sleep(2)
            
            # Debug: Check page content
            page_title = await page.title()
            page_content = await page.content()
            print(f"üìÑ Page title: {page_title}")
            
            # Check pagination ƒë·ªÉ detect s·ªë trang cu·ªëi c√πng (ch·ªâ check, kh√¥ng return ngay)
            max_page_from_pagination = await page.evaluate("""
                () => {
                    // T√¨m pagination v√† l·∫•y s·ªë trang cu·ªëi c√πng
                    const pagination = document.querySelector('.pagination, [class*="pagination"]');
                    if (pagination) {
                        const pageLinks = pagination.querySelectorAll('a, button, [class*="page"]');
                        let maxPage = 0;
                        pageLinks.forEach(link => {
                            const text = link.textContent?.trim() || '';
                            const pageNum = parseInt(text);
                            if (!isNaN(pageNum) && pageNum > maxPage) {
                                maxPage = pageNum;
                            }
                        });
                        return maxPage;
                    }
                    return 0;
                }
            """)
            
            if max_page_from_pagination > 0:
                print(f"üìä S·ªë trang t·ªëi ƒëa t·ª´ pagination: {max_page_from_pagination}")
                if page_number > max_page_from_pagination:
                    print(f"‚ö†Ô∏è  Trang {page_number} v∆∞·ª£t qu√° s·ªë trang t·ªëi ƒëa ({max_page_from_pagination}), d·ª´ng l·∫°i")
                    return []
            
            # Extract movies data tr∆∞·ªõc, sau ƒë√≥ m·ªõi check "kh√¥ng t√¨m th·∫•y"
            # V√¨ c√≥ th·ªÉ text "kh√¥ng t√¨m th·∫•y" n·∫±m ·ªü footer/header nh∆∞ng v·∫´n c√≥ data
            
            # Extract movies data using XPath and CSS selectors
            movies = await page.evaluate("""
                () => {
                    const results = [];
                    
                    // T√¨m container b·∫±ng CSS selector (∆∞u ti√™n h∆°n XPath)
                    let container = document.querySelector('.content-left') || 
                                   document.querySelector('.row.content-row') ||
                                   document.querySelector('[class*="content-row"]');
                    
                    // Fallback: XPath n·∫øu CSS kh√¥ng t√¨m ƒë∆∞·ª£c
                    if (!container) {
                        try {
                            const xpathResult = document.evaluate(
                                '/html/body/div[5]/div[1]/div[3]/div[1]/div[1]/div[1]',
                                document,
                                null,
                                XPathResult.FIRST_ORDERED_NODE_TYPE,
                                null
                            );
                            container = xpathResult.singleNodeValue;
                        } catch (e) {
                            console.log('XPath error:', e);
                        }
                    }
                    
                    console.log('Container found:', !!container);
                    
                    // T√¨m T·∫§T C·∫¢ cards tr·ª±c ti·∫øp t·ª´ container ho·∫∑c to√†n trang
                    let cards = [];
                    
                    if (container) {
                        // T√¨m t·∫•t c·∫£ cards c√≥ class col-6, col-md-4, etc. trong container
                        cards = Array.from(container.querySelectorAll(
                            'div.col-6.col-sm-6.col-md-4, ' +
                            'div[class*="col-6"][class*="col-md-4"], ' +
                            'div[class*="col-"]'
                        ));
                        console.log('Cards from container:', cards.length);
                        
                        // Filter: ch·ªâ l·∫•y cards c√≥ link /phim-sex-chi-tiet/
                        cards = cards.filter(card => {
                            const link = card.querySelector('a[href*="/phim-sex-chi-tiet"]');
                            return link !== null;
                        });
                        console.log('Cards with phim-sex-chi-tiet links:', cards.length);
                    }
                    
                    // Fallback: N·∫øu kh√¥ng c√≥ container ho·∫∑c kh√¥ng t√¨m ƒë∆∞·ª£c cards
                    if (cards.length === 0) {
                        // T√¨m t·∫•t c·∫£ cards c√≥ class col-6 col-md-4 v√† c√≥ link /phim-sex-chi-tiet/
                        cards = Array.from(document.querySelectorAll(
                            'div.col-6.col-sm-6.col-md-4, ' +
                            'div[class*="col-6"][class*="col-md-4"]'
                        )).filter(card => {
                            const link = card.querySelector('a[href*="/phim-sex-chi-tiet"]');
                            return link !== null;
                        });
                        console.log('Cards from page-wide search:', cards.length);
                    }
                    
                    // Fallback 2: T√¨m t·ª´ links v√† l·∫•y parent card
                    if (cards.length === 0) {
                        const allLinks = Array.from(document.querySelectorAll('a[href*="/phim-sex-chi-tiet"]'));
                        console.log('Total phim-sex-chi-tiet links found:', allLinks.length);
                        
                        const uniqueCards = new Set();
                        allLinks.forEach(link => {
                            const href = link.getAttribute('href') || link.href || '';
                            if (href && href.includes('/phim-sex-chi-tiet/')) {
                                let card = link.closest('div[class*="col-"]');
                                if (card) {
                                    uniqueCards.add(card);
                                }
                            }
                        });
                        cards = Array.from(uniqueCards);
                        console.log('Cards from links:', cards.length);
                    }
                    
                    console.log('Final cards count:', cards.length);
                    
                    cards.forEach((card, index) => {
                        try {
                            const movie = {
                                title: '',
                                thumbnail: '',
                                duration: '',
                                views: 0,
                                rating: '',
                                detailUrl: '',
                                category: '',
                                uploadDate: ''
                            };
                            
                            // Extract detail URL - ∆∞u ti√™n t√¨m link /phim-sex/ ho·∫∑c /phim-sex-chi-tiet/
                            let detailUrl = '';
                            
                            // Method 1: N·∫øu card ch√≠nh l√† link
                            if (card.tagName === 'A' && card.href && (card.href.includes('/phim-sex/') || card.href.includes('/phim-sex-chi-tiet/'))) {
                                detailUrl = card.href;
                            }
                            
                            // Method 2: T√¨m link /phim-sex/ ho·∫∑c /phim-sex-chi-tiet/ trong card (∆∞u ti√™n)
                            if (!detailUrl) {
                                const link = card.querySelector('a[href*="/phim-sex"]');
                                if (link) {
                                    detailUrl = link.getAttribute('href') || link.href || '';
                                }
                            }
                            
                            // Method 3: T√¨m t·∫•t c·∫£ links trong card
                            if (!detailUrl) {
                                const allLinks = card.querySelectorAll('a');
                                for (let a of allLinks) {
                                    const href = a.getAttribute('href') || a.href || '';
                                    if (href && (href.includes('/phim-sex/') || href.includes('/phim-sex-chi-tiet/'))) {
                                        detailUrl = href;
                                        break;
                                    }
                                }
                            }
                            
                            // Method 4: T√¨m t·ª´ onclick ho·∫∑c data attributes
                            if (!detailUrl) {
                                const onclick = card.getAttribute('onclick') || '';
                                const match = onclick.match(/\/phim-sex[^"'\s)]+/);
                                if (match) {
                                    detailUrl = match[0];
                                }
                            }
                            
                            // Normalize URL
                            if (detailUrl) {
                                if (detailUrl.startsWith('//')) {
                                    detailUrl = 'https:' + detailUrl;
                                } else if (detailUrl.startsWith('/')) {
                                    detailUrl = 'https://gaigu1.net' + detailUrl;
                                } else if (!detailUrl.startsWith('http')) {
                                    detailUrl = 'https://gaigu1.net/' + detailUrl;
                                }
                                movie.detailUrl = detailUrl;
                            }
                            
                            // Extract thumbnail if not already extracted
                            if (!movie.thumbnail) {
                                
                                // Extract thumbnail image
                                const img = card.querySelector('img');
                                if (img) {
                                    let src = img.getAttribute('src') || img.getAttribute('data-src') || img.src || '';
                                    if (src) {
                                        if (src.startsWith('//')) {
                                            src = 'https:' + src;
                                        } else if (src.startsWith('/')) {
                                            src = 'https://gaigu1.net' + src;
                                        } else if (!src.startsWith('http')) {
                                            src = 'https://gaigu1.net/' + src;
                                        }
                                        movie.thumbnail = src;
                                    }
                                }
                            }
                            
                            // Extract title - ∆∞u ti√™n .content-title (theo HTML structure th·ª±c t·∫ø)
                            const titleSelectors = [
                                '.content-title',  // Ch√≠nh x√°c nh·∫•t theo HTML structure
                                'span.content-title',
                                'a[href*="/phim-sex-chi-tiet"] .content-title',
                                'a[href*="/phim-sex-chi-tiet"] span',
                                'a[href*="/phim-sex-chi-tiet"]',  // Fallback: l·∫•y text t·ª´ link
                            ];
                            
                            for (const selector of titleSelectors) {
                                const titleEl = card.querySelector(selector);
                                if (titleEl) {
                                    let titleText = titleEl.textContent?.trim() || titleEl.getAttribute('title') || '';
                                    // Lo·∫°i b·ªè c√°c text kh√¥ng ph·∫£i title (nh∆∞ "ƒêƒÉng Nh·∫≠p", "ƒêƒÉng k√Ω", etc.)
                                    if (titleText && 
                                        !titleText.toLowerCase().includes('ƒëƒÉng nh·∫≠p') &&
                                        !titleText.toLowerCase().includes('ƒëƒÉng k√Ω') &&
                                        !titleText.toLowerCase().includes('login') &&
                                        !titleText.toLowerCase().includes('register') &&
                                        titleText.length > 3) {  // Title ph·∫£i c√≥ √≠t nh·∫•t 3 k√Ω t·ª±
                                        movie.title = titleText;
                                        break;
                                    }
                                }
                            }
                            
                            // N·∫øu v·∫´n ch∆∞a c√≥ title, th·ª≠ l·∫•y t·ª´ img alt ho·∫∑c title attribute
                            if (!movie.title) {
                                const img = card.querySelector('img');
                                if (img) {
                                    const imgTitle = img.getAttribute('title') || img.getAttribute('alt') || '';
                                    if (imgTitle && 
                                        !imgTitle.toLowerCase().includes('ƒëƒÉng nh·∫≠p') &&
                                        !imgTitle.toLowerCase().includes('login') &&
                                        imgTitle.length > 3) {
                                        movie.title = imgTitle.trim();
                                    }
                                }
                            }
                            
                            // Extract duration - look for time format like "02:20", "00:26"
                            const durationSelectors = [
                                '[class*="duration"]',
                                '[class*="time"]',
                                '.video-duration',
                                '[style*="position: absolute"]',
                                '[class*="overlay"]'
                            ];
                            for (const selector of durationSelectors) {
                                const durationEl = card.querySelector(selector);
                                if (durationEl) {
                                    const durationText = durationEl.textContent?.trim();
                                    const durationMatch = durationText.match(/(\\d{1,2}:\\d{2})/);
                                    if (durationMatch) {
                                        movie.duration = durationMatch[1];
                                        break;
                                    }
                                }
                            }
                            
                            // Extract views - look for "X.XK l∆∞·ª£t xem" or "XK l∆∞·ª£t xem"
                            const viewsSelectors = [
                                '[class*="view"]',
                                '[class*="luot-xem"]',
                                '.views',
                                '[class*="viewed"]'
                            ];
                            for (const selector of viewsSelectors) {
                                const viewsEl = card.querySelector(selector);
                                if (viewsEl) {
                                    const viewsText = viewsEl.textContent?.trim() || '';
                                    // Extract views (e.g., "1.8K l∆∞·ª£t xem" -> 1800, "35.2K" -> 35200)
                                    const viewsMatch = viewsText.match(/(\\d+[.,]?\\d*)\\s*K/i);
                                    if (viewsMatch) {
                                        const num = parseFloat(viewsMatch[1].replace(',', '.'));
                                        movie.views = Math.round(num * 1000);
                                        break;
                                    } else {
                                        const numMatch = viewsText.match(/(\\d+)/);
                                        if (numMatch) {
                                            movie.views = parseInt(numMatch[1]);
                                            break;
                                        }
                                    }
                                }
                            }
                            
                            // Extract rating (e.g., "100%" with thumbs up)
                            const ratingSelectors = [
                                '[class*="rating"]',
                                '[class*="percent"]',
                                '.rating',
                                '[class*="thumbs"]'
                            ];
                            for (const selector of ratingSelectors) {
                                const ratingEl = card.querySelector(selector);
                                if (ratingEl) {
                                    movie.rating = ratingEl.textContent?.trim() || '';
                                    if (movie.rating) break;
                                }
                            }
                            
                            // Extract category/tags
                            const categoryEl = card.querySelector('[class*="category"], [class*="tag"], .category');
                            if (categoryEl) {
                                movie.category = categoryEl.textContent?.trim() || '';
                            }
                            
                            // Only add if we have at least title and thumbnail
                            if (movie.title && movie.title.length > 0 && movie.thumbnail) {
                                results.push(movie);
                            }
                        } catch (e) {
                            console.error('Error extracting movie:', e);
                        }
                    });
                    
                    return results;
                }
            """)
            
            # Debug: Print raw results
            print(f"üìä Raw results: {len(movies)} items")
            if len(movies) > 0:
                print(f"üìù Sample: {json.dumps(movies[0], ensure_ascii=False, indent=2)[:200]}...")
            
            # Filter and limit - relax requirements
            valid_movies = []
            for m in movies:
                # Ch·∫•p nh·∫≠n n·∫øu c√≥ detailUrl ho·∫∑c c√≥ title
                if (m.get('detailUrl') or m.get('title')) and m.get('thumbnail'):
                    valid_movies.append(m)
                elif m.get('detailUrl'):  # Ch·∫•p nh·∫≠n n·∫øu c√≥ detailUrl d√π kh√¥ng c√≥ thumbnail
                    valid_movies.append(m)
            
            if limit:
                valid_movies = valid_movies[:limit]
            
            print(f"‚úÖ ƒê√£ crawl ƒë∆∞·ª£c {len(valid_movies)} phim t·ª´ {len(movies)} items")
            
            # N·∫øu kh√¥ng c√≥ movies, check xem c√≥ ph·∫£i do "kh√¥ng t√¨m th·∫•y" kh√¥ng
            if len(valid_movies) == 0:
                # Check trong content area xem c√≥ th√¥ng b√°o "kh√¥ng t√¨m th·∫•y"
                has_no_results = await page.evaluate("""
                    () => {
                        // T√¨m content area ch√≠nh (kh√¥ng ph·∫£i footer/header)
                        const contentArea = document.querySelector('.content-row, .row.content-row, [class*="content-row"], main, .main-content, #content, .content');
                        if (contentArea) {
                            const text = contentArea.textContent?.toLowerCase() || '';
                            // Check xem c√≥ text "kh√¥ng t√¨m th·∫•y" v√† kh√¥ng c√≥ cards
                            const hasNoResultsText = text.includes('kh√¥ng t√¨m th·∫•y') || text.includes('kh√¥ng c√≥') || text.includes('no results');
                            // Check xem c√≥ cards kh√¥ng
                            const hasCards = contentArea.querySelectorAll('a[href*="/phim-sex/"]').length > 0;
                            return hasNoResultsText && !hasCards;
                        }
                        return false;
                    }
                """)
                
                if has_no_results:
                    print(f"‚ö†Ô∏è  Trang {page_number} c√≥ th√¥ng b√°o 'kh√¥ng t√¨m th·∫•y' v√† kh√¥ng c√≥ cards")
                elif len(movies) > 0:
                    print(f"‚ö†Ô∏è  C√≥ {len(movies)} items nh∆∞ng kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán (c·∫ßn detailUrl ho·∫∑c title + thumbnail)")
                    print(f"üìã Sample item: {json.dumps(movies[0] if movies else {}, ensure_ascii=False)}")
                else:
                    print(f"‚ö†Ô∏è  Trang {page_number} kh√¥ng t√¨m th·∫•y movies (c√≥ th·ªÉ do selector sai ho·∫∑c bot detection)")
                    # Debug: In ra m·ªôt s·ªë th√¥ng tin v·ªÅ page
                    debug_info = await page.evaluate("""
                        () => {
                            return {
                                hasPhimSexLinks: document.querySelectorAll('a[href*="/phim-sex/"]').length,
                                hasColElements: document.querySelectorAll('[class*="col-"]').length,
                                hasImages: document.querySelectorAll('img').length,
                                bodyTextLength: document.body?.textContent?.length || 0
                            };
                        }
                    """)
                    print(f"üìä Debug info: {json.dumps(debug_info, ensure_ascii=False)}")
            else:
                # In sample movie ƒë·ªÉ debug
                if len(valid_movies) > 0:
                    print(f"üìã Sample movie: {valid_movies[0].get('title', 'N/A')[:50]}...")
            
            return valid_movies
            
        except Exception as e:
            print(f"‚ùå L·ªói khi crawl movies list: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
        finally:
            await page.close()
    
    def save_to_json(self, movies: List[Dict], filename: str = None) -> Dict:
        """L∆∞u v√†o file JSON"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawled_movies_{timestamp}.json"
        
        # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
        data_dir = os.path.join(os.path.dirname(__file__), "data")
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(movies, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ ƒê√£ l∆∞u {len(movies)} phim v√†o {filepath}")
        return {"saved": len(movies), "file": filepath}
    
    async def crawl_all_listing_pages(self, start_page: int = 1, max_pages: int = None, save_interval: int = 50, concurrent_pages: int = 3):
        """Crawl t·∫•t c·∫£ c√°c trang listing
        
        Args:
            start_page: Trang b·∫Øt ƒë·∫ßu
            max_pages: S·ªë trang t·ªëi ƒëa (None = t·ª± ƒë·ªông detect)
            save_interval: L∆∞u file sau m·ªói N trang (m·∫∑c ƒë·ªãnh: 50)
        
        Returns:
            Dict: {"movies": List[Dict], "listing_file": str}
        """
        all_movies = []
        current_page = start_page
        
        # T·∫°o filename v·ªõi timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        listing_file = f"listing_movies_{timestamp}.json"
        
        print(f"\n{'='*50}")
        print(f"üìã GIAI ƒêO·∫†N 1: Crawl listing pages")
        if max_pages:
            print(f"   T·ª´ trang {start_page} ƒë·∫øn {start_page + max_pages - 1}")
        else:
            print(f"   T·ª´ trang {start_page} (t·ª± ƒë·ªông detect s·ªë trang)")
        print(f"   üíæ L∆∞u sau m·ªói {save_interval} trang")
        print(f"   üìÅ File: {listing_file}")
        print(f"{'='*50}\n")
        
        consecutive_empty = 0  # ƒê·∫øm s·ªë trang r·ªóng li√™n ti·∫øp
        
        while True:
            if max_pages and current_page > start_page + max_pages - 1:
                break
            
            print(f"\nüìÑ Trang {current_page}")
            movies = await self.crawl_movies_list(current_page, 60)
            
            # Ki·ªÉm tra n·∫øu trang r·ªóng
            if not movies or len(movies) == 0:
                consecutive_empty += 1
                print(f"‚ö†Ô∏è  Trang {current_page} kh√¥ng c√≥ d·ªØ li·ªáu (l·∫ßn {consecutive_empty})")
                
                # N·∫øu 2 trang li√™n ti·∫øp r·ªóng ‚Üí h·∫øt trang
                if consecutive_empty >= 2:
                    print(f"üõë ƒê√£ h·∫øt trang (2 trang li√™n ti·∫øp r·ªóng), d·ª´ng l·∫°i")
                    break
            else:
                consecutive_empty = 0  # Reset counter n·∫øu c√≥ data
                all_movies.extend(movies)
                print(f"‚úÖ ƒê√£ c√≥ t·ªïng c·ªông {len(all_movies)} phim\n")
                
                # L∆∞u theo interval ƒë·ªÉ kh√¥ng m·∫•t data n·∫øu stop
                if current_page % save_interval == 0:
                    print(f"üíæ ƒêang l∆∞u checkpoint (sau {current_page} trang)...")
                    result = self.save_to_json(all_movies, listing_file)
                    print(f"‚úÖ ƒê√£ l∆∞u {len(all_movies)} phim v√†o {result.get('file', '')}\n")
            
            # Delay between pages (gi·∫£m xu·ªëng ƒë·ªÉ nhanh h∆°n)
            if consecutive_empty == 0:  # Ch·ªâ delay n·∫øu c√≥ data
                print(f"‚è≥ ƒê·ª£i 2 gi√¢y tr∆∞·ªõc khi crawl trang ti·∫øp theo...\n")
                await asyncio.sleep(2)  # Gi·∫£m t·ª´ 5s xu·ªëng 2s
            else:
                # Delay ng·∫Øn h∆°n n·∫øu trang r·ªóng (c√≥ th·ªÉ ƒëang check)
                await asyncio.sleep(1)
            
            current_page += 1
        
        # L∆∞u l·∫ßn cu·ªëi (t·∫•t c·∫£ data)
        print(f"üíæ ƒêang l∆∞u file cu·ªëi c√πng...")
        result = self.save_to_json(all_movies, listing_file)
        
        print(f"\n‚úÖ Ho√†n th√†nh crawl listing: {len(all_movies)} phim")
        print(f"üíæ ƒê√£ l∆∞u danh s√°ch v√†o: {result.get('file', '')}\n")
        
        return {
            "movies": all_movies,
            "listing_file": result.get("file", "")
        }
    
    async def _crawl_one_movie_detail(self, movie: Dict, index: int, total: int, save_individual: bool = True, save_combined: bool = False, combined_file: str = None, all_details: list = None) -> tuple:
        """Crawl detail cho 1 phim (d√πng trong concurrent crawling)
        
        Returns:
            (success: bool, movie: Dict)
        """
        async with self.semaphore:  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng concurrent
            if not movie.get('detailUrl'):
                return (False, movie)
            
            movie_title = movie.get('title', 'N/A')[:40]
            print(f"[{index}/{total}] üîç ƒêang crawl: {movie_title}...")
            
            try:
                # Random delay ƒë·ªÉ tr√°nh pattern detection
                delay = random.uniform(self.delay_min, self.delay_max)
                await asyncio.sleep(delay)
                
                detail_data = await self.crawl_movie_detail(movie['detailUrl'])
                if detail_data:
                    # Merge detail data
                    detail_url = movie.get('detailUrl')
                    movie.update(detail_data)
                    movie['detailUrl'] = detail_url
                    
                    # L∆∞u v√†o file ri√™ng n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
                    if save_individual:
                        filepath = self.save_movie_detail_to_file(movie)
                        if filepath:
                            print(f"[{index}/{total}] ‚úÖ {movie_title[:30]}... ‚Üí {os.path.basename(filepath)}")
                        else:
                            print(f"[{index}/{total}] ‚ö†Ô∏è  ƒê√£ crawl nh∆∞ng kh√¥ng l∆∞u ƒë∆∞·ª£c file")
                    
                    # L∆∞u v√†o combined file n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu (incremental)
                    if save_combined and combined_file and all_details is not None:
                        async with self.file_lock:  # Thread-safe
                            all_details.append(movie)
                            # L∆∞u ngay sau m·ªói item
                            try:
                                with open(combined_file, 'w', encoding='utf-8') as f:
                                    json.dump(all_details, f, ensure_ascii=False, indent=2)
                            except Exception as e:
                                print(f"‚ö†Ô∏è  L·ªói khi l∆∞u combined file: {e}")
                    
                    return (True, movie)
                else:
                    print(f"[{index}/{total}] ‚ö†Ô∏è  Kh√¥ng crawl ƒë∆∞·ª£c: {movie_title[:30]}...")
                    return (False, movie)
            except Exception as e:
                print(f"[{index}/{total}] ‚ùå L·ªói: {movie_title[:30]}... - {str(e)}")
                return (False, movie)
    
    async def crawl_details_from_listing_file(self, listing_file: str, save_individual: bool = True, batch_size: int = None, save_combined: bool = False):
        """ƒê·ªçc file listing v√† crawl detail cho t·ª´ng phim (concurrent)
        
        Args:
            listing_file: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ch·ª©a danh s√°ch phim
            save_individual: N·∫øu True, l∆∞u m·ªói phim v√†o file ri√™ng v·ªõi t√™n phim
            batch_size: S·ªë l∆∞·ª£ng phim crawl m·ªói batch (None = t·∫•t c·∫£ c√πng l√∫c)
            save_combined: N·∫øu True, g·ªôm t·∫•t c·∫£ v√†o 1 JSON file v√† l∆∞u incremental
        """
        # ƒê·ªçc file listing
        if not os.path.exists(listing_file):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {listing_file}")
            return None
        
        with open(listing_file, 'r', encoding='utf-8') as f:
            movies = json.load(f)
        
        # L·ªçc nh·ªØng phim c√≥ detailUrl
        valid_movies = [(i, movie) for i, movie in enumerate(movies, 1) if movie.get('detailUrl')]
        
        print(f"\n{'='*50}")
        print(f"üîç GIAI ƒêO·∫†N 2: Crawl detail cho {len(valid_movies)} phim")
        print(f"   T·ª´ file: {listing_file}")
        print(f"   L∆∞u ri√™ng t·ª´ng file: {save_individual}")
        print(f"   G·ªôm v√†o 1 JSON: {save_combined}")
        print(f"   Concurrent: {self.max_concurrent} requests")
        print(f"   Delay: {self.delay_min}-{self.delay_max} gi√¢y")
        if batch_size:
            print(f"   Batch size: {batch_size}")
        print(f"{'='*50}\n")
        
        success_count = 0
        failed_count = 0
        
        # T·∫°o file combined n·∫øu c·∫ßn
        combined_file = None
        all_details = []
        if save_combined:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            combined_file = os.path.join("data", f"all_movies_details_{timestamp}.json")
            os.makedirs("data", exist_ok=True)
            print(f"üíæ File g·ªôm: {combined_file}\n")
        
        # Crawl theo batch ho·∫∑c t·∫•t c·∫£ c√πng l√∫c
        if batch_size:
            # Crawl theo batch
            for batch_start in range(0, len(valid_movies), batch_size):
                batch_end = min(batch_start + batch_size, len(valid_movies))
                batch = valid_movies[batch_start:batch_end]
                
                print(f"\nüì¶ Batch {batch_start//batch_size + 1}: {len(batch)} phim\n")
                
                # T·∫°o tasks cho batch n√†y
                tasks = [
                    self._crawl_one_movie_detail(movie, index, len(valid_movies), save_individual, save_combined, combined_file, all_details)
                    for index, movie in batch
                ]
                
                # Ch·ªù t·∫•t c·∫£ tasks trong batch ho√†n th√†nh
                results = await asyncio.gather(*tasks)
                
                # ƒê·∫øm k·∫øt qu·∫£
                for success, updated_movie in results:
                    if success:
                        success_count += 1
                    else:
                        failed_count += 1
                
                # Delay gi·ªØa c√°c batch
                if batch_end < len(valid_movies):
                    batch_delay = random.uniform(5, 10)
                    print(f"\n‚è≥ ƒê·ª£i {batch_delay:.1f} gi√¢y tr∆∞·ªõc batch ti·∫øp theo...\n")
                    await asyncio.sleep(batch_delay)
        else:
            # Crawl t·∫•t c·∫£ c√πng l√∫c (gi·ªõi h·∫°n b·ªüi semaphore)
            print(f"üöÄ B·∫Øt ƒë·∫ßu crawl {len(valid_movies)} phim (concurrent: {self.max_concurrent})\n")
            
            tasks = [
                self._crawl_one_movie_detail(movie, index, len(valid_movies), save_individual, save_combined, combined_file, all_details)
                for index, movie in valid_movies
            ]
            
            results = await asyncio.gather(*tasks)
            
            # ƒê·∫øm k·∫øt qu·∫£
            for success, updated_movie in results:
                if success:
                    success_count += 1
                else:
                    failed_count += 1
        
        # L∆∞u t·∫•t c·∫£ v√†o 1 file t·ªïng h·ª£p (n·∫øu c·∫ßn)
        if not save_individual:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            all_details_file = f"all_movie_details_{timestamp}.json"
            result = self.save_to_json(movies, all_details_file)
            print(f"\nüíæ ƒê√£ l∆∞u t·∫•t c·∫£ detail v√†o: {result.get('file', '')}")
        
        print(f"\n{'='*50}")
        print(f"‚úÖ HO√ÄN TH√ÄNH CRAWL DETAIL")
        print(f"   ‚úÖ Th√†nh c√¥ng: {success_count}")
        print(f"   ‚ùå Th·∫•t b·∫°i: {failed_count}")
        if len(valid_movies) > 0:
            print(f"   üìä T·ª∑ l·ªá: {success_count/len(valid_movies)*100:.1f}%")
        print(f"{'='*50}\n")
        
        return {
            "total": len(valid_movies),
            "success": success_count,
            "failed": failed_count
        }
    
    async def crawl_movie_detail(self, detail_url: str) -> Optional[Dict]:
        """Crawl chi ti·∫øt phim t·ª´ detail page
        
        Args:
            detail_url: URL c·ªßa trang detail phim
            
        Returns:
            Dict ch·ª©a th√¥ng tin chi ti·∫øt phim ho·∫∑c None n·∫øu l·ªói
        """
        await self.init_browser()
        
        print(f"üîç ƒêang crawl detail: {detail_url}")
        
        page = await self.browser.new_page()
        
        try:
            # Set realistic headers
            await page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                'Referer': 'https://gaigu1.net/phim-sex',
            })
            
            await page.goto(detail_url, wait_until='networkidle', timeout=60000)
            await asyncio.sleep(3)  # Wait for video player to load
            
            # Check for captcha
            page_content = await page.content()
            if 'verify' in page_content.lower() and 'human' in page_content.lower():
                print("‚ö†Ô∏è  Ph√°t hi·ªán captcha, ƒë·ª£i th√™m...")
                await asyncio.sleep(5)
                await page.wait_for_selector('video, .video-player, [class*="video"]', timeout=10000)
            
            # Extract video details
            movie_detail = await page.evaluate("""
                () => {
                    const data = {
                        title: '',
                        description: '',
                        videoUrl: '',
                        videoSources: [],
                        poster: '',
                        duration: '',
                        views: 0,
                        rating: '',
                        category: '',
                        tags: [],
                        uploadDate: ''
                    };
                    
                    // Extract title - ∆∞u ti√™n c√°c selector ch√≠nh x√°c
                    const titleSelectors = [
                        'h1',
                        '.content-title',
                        '[class*="content-title"]',
                        'h2',
                        '.title',
                        '[class*="title"]:not([class*="login"]):not([class*="register"])'
                    ];
                    
                    for (const selector of titleSelectors) {
                        const titleEl = document.querySelector(selector);
                        if (titleEl) {
                            let titleText = titleEl.textContent?.trim() || '';
                            // Lo·∫°i b·ªè c√°c text kh√¥ng ph·∫£i title
                            if (titleText && 
                                !titleText.toLowerCase().includes('ƒëƒÉng nh·∫≠p') &&
                                !titleText.toLowerCase().includes('ƒëƒÉng k√Ω') &&
                                !titleText.toLowerCase().includes('login') &&
                                !titleText.toLowerCase().includes('register') &&
                                !titleText.toLowerCase().includes('sign in') &&
                                !titleText.toLowerCase().includes('sign up') &&
                                titleText.length > 3) {
                                data.title = titleText;
                                break;
                            }
                        }
                    }
                    
                    // Fallback: L·∫•y t·ª´ URL n·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c title
                    if (!data.title || data.title.length < 3) {
                        const urlMatch = window.location.pathname.match(/phim-sex-chi-tiet\/\d+\/(.+)/);
                        if (urlMatch && urlMatch[1]) {
                            // Decode URL v√† format l·∫°i
                            data.title = decodeURIComponent(urlMatch[1]).replace(/-/g, ' ').trim();
                        }
                    }
                    
                    // Extract video element - ∆∞u ti√™n video#video_html5_api
                    let videoEl = document.querySelector('video#video_html5_api');
                    
                    // Fallback: T√¨m b·∫±ng XPath n·∫øu CSS selector kh√¥ng t√¨m ƒë∆∞·ª£c
                    if (!videoEl) {
                        try {
                            const xpathResult = document.evaluate(
                                '/html/body/div[5]/div[3]/div[2]/div[1]/div[1]/div/video',
                                document,
                                null,
                                XPathResult.FIRST_ORDERED_NODE_TYPE,
                                null
                            );
                            videoEl = xpathResult.singleNodeValue;
                        } catch (e) {
                            console.log('XPath error:', e);
                        }
                    }
                    
                    // Fallback 2: T√¨m b·∫•t k·ª≥ video element n√†o
                    if (!videoEl) {
                        videoEl = document.querySelector('video.vjs-tech, video');
                    }
                    
                    if (videoEl) {
                        console.log('Video element found:', videoEl.id, videoEl.className);
                        // Helper function ƒë·ªÉ normalize URL v√† validate
                        function normalizeVideoUrl(url) {
                            if (!url) return null;
                            
                            // N·∫øu ƒë√£ l√† full URL (c√≥ http/https), gi·ªØ nguy√™n
                            if (url.startsWith('http://') || url.startsWith('https://')) {
                                return url;
                            }
                            
                            // N·∫øu l√† protocol-relative (//domain.com/...), th√™m https:
                            if (url.startsWith('//')) {
                                return 'https:' + url;
                            }
                            
                            // N·∫øu l√† relative URL, th√™m domain m·∫∑c ƒë·ªãnh (nh∆∞ng th∆∞·ªùng video s·∫Ω l√† full URL)
                            // Trong tr∆∞·ªùng h·ª£p n√†y, video th∆∞·ªùng l√† full URL t·ª´ CDN
                            return 'https://gaigu1.net' + url;
                        }
                        
                        // Get main video URL from src attribute (ƒë√¢y l√† video ch√≠nh)
                        const videoSrc = videoEl.getAttribute('src');
                        if (videoSrc) {
                            const normalizedUrl = normalizeVideoUrl(videoSrc);
                            if (normalizedUrl && normalizedUrl.includes('.mp4')) {
                                data.videoUrl = normalizedUrl;
                            }
                        }
                        
                        // Get poster image (thumbnail c·ªßa video player)
                        const poster = videoEl.getAttribute('poster');
                        if (poster) {
                            data.poster = normalizeVideoUrl(poster);
                        }
                        
                        // Get all source elements for different qualities (720p, 480p, 360p)
                        const sources = videoEl.querySelectorAll('source');
                        console.log('Found sources:', sources.length);
                        
                        sources.forEach((source, index) => {
                            const src = source.getAttribute('src');
                            const type = source.getAttribute('type') || 'video/mp4';
                            const label = source.getAttribute('label') || '';
                            const res = source.getAttribute('res') || '';
                            
                            if (src) {
                                // Normalize URL - gi·ªØ nguy√™n domain t·ª´ HTML
                                const fullUrl = normalizeVideoUrl(src);
                                if (fullUrl && fullUrl.includes('.mp4')) {
                                    // T·∫°o label n·∫øu kh√¥ng c√≥ (t·ª´ res ho·∫∑c t·ª´ URL)
                                    let qualityLabel = label || res || '';
                                    if (!qualityLabel && src.includes('_')) {
                                        // Extract quality t·ª´ filename: 77793_720p.mp4 -> 720p
                                        const qualityMatch = src.match(/_(\d+p)\.mp4/);
                                        if (qualityMatch) {
                                            qualityLabel = qualityMatch[1];
                                        }
                                    }
                                    
                                    data.videoSources.push({
                                        url: fullUrl,
                                        type: type,
                                        label: qualityLabel,
                                        resolution: res || qualityLabel
                                    });
                                    console.log(`Source ${index + 1}: ${qualityLabel} - ${fullUrl}`);
                                }
                            }
                        });
                        
                        // Sort sources by resolution (highest first)
                        data.videoSources.sort((a, b) => {
                            const resA = parseInt(a.resolution || a.label || '0');
                            const resB = parseInt(b.resolution || b.label || '0');
                            return resB - resA;
                        });
                        
                        // N·∫øu c√≥ videoUrl nh∆∞ng ch∆∞a c√≥ trong videoSources, th√™m v√†o
                        if (data.videoUrl && data.videoSources.length === 0) {
                            data.videoSources.push({
                                url: data.videoUrl,
                                type: 'video/mp4',
                                label: 'SD',
                                resolution: '360'
                            });
                        }
                        
                        // N·∫øu videoUrl ch∆∞a c√≥ nh∆∞ng c√≥ sources, l·∫•y source ƒë·∫ßu ti√™n l√†m videoUrl
                        if (!data.videoUrl && data.videoSources.length > 0) {
                            data.videoUrl = data.videoSources[0].url;
                        }
                        
                        // Debug: Log domain ƒë·ªÉ verify
                        if (data.videoUrl) {
                            try {
                                const urlObj = new URL(data.videoUrl);
                                console.log('Video domain:', urlObj.hostname);
                            } catch (e) {
                                console.log('Video URL:', data.videoUrl);
                            }
                        }
                    }
                    
                    // Extract description
                    const descEl = document.querySelector('[class*="description"], [class*="content"], .content p, [class*="bio"]');
                    if (descEl) {
                        data.description = descEl.textContent?.trim() || '';
                    }
                    
                    // Extract views
                    const viewsEl = document.querySelector('[class*="view"], [class*="luot-xem"], .views');
                    if (viewsEl) {
                        const viewsText = viewsEl.textContent?.trim() || '';
                        const viewsMatch = viewsText.match(/(\\d+[.,]?\\d*)\\s*K/i);
                        if (viewsMatch) {
                            const num = parseFloat(viewsMatch[1].replace(',', '.'));
                            data.views = Math.round(num * 1000);
                        } else {
                            const numMatch = viewsText.match(/(\\d+)/);
                            if (numMatch) {
                                data.views = parseInt(numMatch[1]);
                            }
                        }
                    }
                    
                    // Extract rating
                    const ratingEl = document.querySelector('[class*="rating"], [class*="percent"]');
                    if (ratingEl) {
                        data.rating = ratingEl.textContent?.trim() || '';
                    }
                    
                    // Extract category
                    const categoryEl = document.querySelector('[class*="category"], [class*="tag"], .category');
                    if (categoryEl) {
                        data.category = categoryEl.textContent?.trim() || '';
                    }
                    
                    // Extract tags
                    document.querySelectorAll('[class*="tag"], .hashtag, a[href*="tag"]').forEach(tag => {
                        const tagText = tag.textContent?.trim();
                        if (tagText && tagText.length > 0 && tagText.length < 50) {
                            data.tags.push(tagText);
                        }
                    });
                    
                    // Extract upload date
                    const dateEl = document.querySelector('[class*="date"], [class*="upload"], .date');
                    if (dateEl) {
                        data.uploadDate = dateEl.textContent?.trim() || '';
                    }
                    
                    return data;
                }
            """)
            
            if movie_detail.get('videoUrl') or movie_detail.get('videoSources'):
                print(f"‚úÖ ƒê√£ crawl detail: {movie_detail.get('title', 'N/A')[:40]}... - {len(movie_detail.get('videoSources', []))} quality")
                return movie_detail
            else:
                print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y video URL")
                return None
                
        except Exception as e:
            print(f"‚ùå L·ªói khi crawl detail {detail_url}: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            await page.close()
    
    def save_movie_detail_to_file(self, movie: Dict) -> str:
        """L∆∞u detail c·ªßa 1 phim v√†o file ri√™ng v·ªõi t√™n l√† title"""
        if not movie.get('title'):
            return None
        
        # Sanitize title ƒë·ªÉ l√†m filename
        filename = self.sanitize_filename(movie['title'])
        filename = f"{filename}.json"
        
        # T·∫°o th∆∞ m·ª•c details n·∫øu ch∆∞a c√≥
        data_dir = os.path.join(os.path.dirname(__file__), "data", "movie_details")
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(movie, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def sanitize_filename(self, name: str) -> str:
        """Chuy·ªÉn t√™n phim th√†nh filename h·ª£p l·ªá"""
        # Lo·∫°i b·ªè emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        name = re.sub(r'[^\w\s-]', '', name)
        # Thay kho·∫£ng tr·∫Øng b·∫±ng d·∫•u g·∫°ch d∆∞·ªõi
        name = re.sub(r'\s+', '_', name)
        # Lo·∫°i b·ªè d·∫•u g·∫°ch d∆∞·ªõi li√™n ti·∫øp
        name = re.sub(r'_+', '_', name)
        # Lo·∫°i b·ªè d·∫•u g·∫°ch d∆∞·ªõi ·ªü ƒë·∫ßu v√† cu·ªëi
        name = name.strip('_')
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        if len(name) > 100:
            name = name[:100]
        # N·∫øu r·ªóng, d√πng t√™n m·∫∑c ƒë·ªãnh
        if not name:
            name = "unknown"
        return name


async def main():
    """Main function"""
    import sys
    
    # Parse concurrent settings
    max_concurrent = 5  # TƒÉng t·ª´ 3 l√™n 5
    delay_min = 1.0     # Gi·∫£m t·ª´ 2.0 xu·ªëng 1.0
    delay_max = 2.0     # Gi·∫£m t·ª´ 5.0 xu·ªëng 2.0
    batch_size = None
    
    for i, arg in enumerate(sys.argv):
        if arg == '--concurrent' and i + 1 < len(sys.argv):
            max_concurrent = int(sys.argv[i + 1])
        elif arg == '--delay-min' and i + 1 < len(sys.argv):
            delay_min = float(sys.argv[i + 1])
        elif arg == '--delay-max' and i + 1 < len(sys.argv):
            delay_max = float(sys.argv[i + 1])
        elif arg == '--batch-size' and i + 1 < len(sys.argv):
            batch_size = int(sys.argv[i + 1])
    
    crawler = MovieCrawler(max_concurrent=max_concurrent, delay_min=delay_min, delay_max=delay_max)
    
    try:
        # Parse flags v√† lo·∫°i b·ªè ch√∫ng kh·ªèi args
        detail_from_file = None
        used_indices = set()  # Track c√°c index ƒë√£ d√πng cho flags
        
        for i, arg in enumerate(sys.argv):
            if arg in ['--detail-from-file', '--from-file'] and i + 1 < len(sys.argv):
                detail_from_file = sys.argv[i + 1]
                used_indices.add(i)
                used_indices.add(i + 1)
                break
            elif arg in ['--concurrent', '--delay-min', '--delay-max', '--batch-size']:
                used_indices.add(i)
                if i + 1 < len(sys.argv):
                    used_indices.add(i + 1)
        
        save_individual = '--save-individual' in sys.argv or '--individual' in sys.argv
        save_combined = '--save-combined' in sys.argv or '--combined' in sys.argv or '--gop' in sys.argv
        auto_mode = '--auto' in sys.argv or '--all' in sys.argv
        listing_only = '--listing-only' in sys.argv
        
        # Args ch·ªâ ch·ª©a c√°c s·ªë trang, kh√¥ng ch·ª©a flags v√† gi√° tr·ªã c·ªßa flags
        # Lo·∫°i b·ªè t·∫•t c·∫£ flags v√† gi√° tr·ªã c·ªßa ch√∫ng
        args = []
        skip_next = False
        flag_with_value = ['--concurrent', '--delay-min', '--delay-max', '--batch-size', '--detail-from-file', '--from-file']
        
        for i, arg in enumerate(sys.argv[1:], 1):
            if skip_next:
                skip_next = False
                continue
            
            # N·∫øu l√† flag c√≥ gi√° tr·ªã, skip gi√° tr·ªã ti·∫øp theo
            if arg in flag_with_value:
                skip_next = True
                continue
            
            # N·∫øu l√† flag kh√°c (--auto, --all, etc.), b·ªè qua
            if arg.startswith('--') or arg.startswith('-'):
                continue
            
            # Ch·ªâ th√™m n·∫øu kh√¥ng ph·∫£i flag v√† kh√¥ng ph·∫£i gi√° tr·ªã c·ªßa flag
            # V√† ph·∫£i l√† s·ªë nguy√™n (s·ªë trang)
            if i not in used_indices:
                try:
                    # Th·ª≠ parse ƒë·ªÉ ki·ªÉm tra xem c√≥ ph·∫£i s·ªë nguy√™n kh√¥ng
                    int(arg)
                    args.append(arg)
                except ValueError:
                    # Kh√¥ng ph·∫£i s·ªë nguy√™n, b·ªè qua (c√≥ th·ªÉ l√† gi√° tr·ªã float c·ªßa flag nh∆∞ "1.0")
                    pass
        
        # Mode 1: Crawl detail t·ª´ file listing
        if detail_from_file:
            print(f"üöÄ CHI·∫æN L∆Ø·ª¢C: Crawl detail t·ª´ file listing")
            print(f"   üìÅ File: {detail_from_file}")
            print(f"   üíæ L∆∞u ri√™ng t·ª´ng file: {save_individual}")
            print(f"   üíæ G·ªôm v√†o 1 JSON: {save_combined}")
            print(f"   üîÑ Concurrent: {max_concurrent}")
            print(f"   ‚è±Ô∏è  Delay: {delay_min}-{delay_max}s")
            if batch_size:
                print(f"   üì¶ Batch size: {batch_size}")
            print()
            result = await crawler.crawl_details_from_listing_file(detail_from_file, save_individual, batch_size, save_combined)
            print(f"\n{'='*50}")
            print("‚úÖ HO√ÄN TH√ÄNH!")
            print(f"{'='*50}")
            if result:
                print(f"üìä T·ªïng: {result.get('total', 0)}")
                print(f"‚úÖ Th√†nh c√¥ng: {result.get('success', 0)}")
                print(f"‚ùå Th·∫•t b·∫°i: {result.get('failed', 0)}")
            return
        
        if len(args) > 0:
            if len(args) >= 2:
                start_page = int(args[0])
                end_page = int(args[1])
                print(f"üöÄ Crawl phim t·ª´ trang {start_page} ƒë·∫øn {end_page}\n")
                all_movies = []
                for page_num in range(start_page, end_page + 1):
                    print(f"\nüìÑ Trang {page_num}/{end_page}")
                    movies = await crawler.crawl_movies_list(page_num, 60)
                    all_movies.extend(movies)
                    print(f"‚úÖ ƒê√£ c√≥ t·ªïng c·ªông {len(all_movies)} phim\n")
                    if page_num < end_page:
                        await asyncio.sleep(5)
                
                result = crawler.save_to_json(all_movies)
                print(f"\n‚úÖ Ho√†n th√†nh: {len(all_movies)} phim")
                print(f"üíæ File: {result.get('file', '')}")
            else:
                page = int(args[0])
                print(f"üöÄ Crawl phim trang {page}\n")
                movies = await crawler.crawl_movies_list(page, 60)
                result = crawler.save_to_json(movies)
                print(f"\n‚úÖ Ho√†n th√†nh: {len(movies)} phim")
                print(f"üíæ File: {result.get('file', '')}")
        else:
            # Auto mode: crawl all pages ‚Üí sau ƒë√≥ crawl detail t·ª´ng video
            if '--auto' in sys.argv or '--all' in sys.argv:
                max_pages = None
                if len(args) > 0:
                    try:
                        max_pages = int(args[0])
                    except:
                        pass
                
                print(f"\n{'='*60}")
                print(f"üöÄ CH·∫æ ƒê·ªò T·ª∞ ƒê·ªòNG: Crawl listing ‚Üí Detail t·ª´ng video")
                print(f"{'='*60}")
                if max_pages:
                    print(f"   üìã S·ªë trang listing t·ªëi ƒëa: {max_pages}")
                else:
                    print(f"   üìã Crawl t·∫•t c·∫£ trang listing (t·ª± ƒë·ªông detect)")
                print(f"   üîç T·ª± ƒë·ªông crawl detail sau khi xong listing")
                print(f"   üíæ L∆∞u ngay sau m·ªói video (t·ªõi ƒë√¢u l∆∞u t·ªõi ƒë√≥)")
                print(f"   üîÑ Concurrent: {max_concurrent}")
                print(f"   ‚è±Ô∏è  Delay: {delay_min}-{delay_max}s")
                if batch_size:
                    print(f"   üì¶ Batch size: {batch_size}")
                print(f"{'='*60}\n")
                
                # Giai ƒëo·∫°n 1: Crawl listing
                listing_result = await crawler.crawl_all_listing_pages(1, max_pages)
                listing_file = listing_result.get('listing_file', '')
                
                if not listing_file or not os.path.exists(listing_file):
                    print("‚ùå Kh√¥ng t√¨m th·∫•y file listing, d·ª´ng l·∫°i")
                    return
                
                print(f"\n{'='*60}")
                print(f"‚úÖ Ho√†n th√†nh crawl listing: {len(listing_result.get('movies', []))} phim")
                print(f"üíæ File: {listing_file}")
                print(f"{'='*60}\n")
                
                # Giai ƒëo·∫°n 2: Crawl detail t·ª´ng video (t·ª± ƒë·ªông)
                print("‚è≥ B·∫Øt ƒë·∫ßu crawl detail sau 3 gi√¢y...\n")
                await asyncio.sleep(3)
                
                detail_result = await crawler.crawl_details_from_listing_file(
                    listing_file, 
                    save_individual=True,  # Lu√¥n l∆∞u ri√™ng t·ª´ng file
                    batch_size=batch_size,
                    save_combined=save_combined
                )
                
                print(f"\n{'='*60}")
                print("‚úÖ HO√ÄN TH√ÄNH T·∫§T C·∫¢!")
                print(f"{'='*60}")
                print(f"üìä Listing: {len(listing_result.get('movies', []))} phim")
                if detail_result:
                    print(f"üìä Detail - Th√†nh c√¥ng: {detail_result.get('success', 0)}")
                    print(f"üìä Detail - Th·∫•t b·∫°i: {detail_result.get('failed', 0)}")
                print(f"üíæ File listing: {listing_file}")
                print(f"üíæ Files detail: data/movie_details/*.json")
                print(f"{'='*60}\n")
                return
            else:
                # Default: crawl page 1
                print(f"üöÄ Crawl phim trang 1 (m·∫∑c ƒë·ªãnh)\n")
                movies = await crawler.crawl_movies_list(1, 60)
                result = crawler.save_to_json(movies)
                print(f"\n‚úÖ Ho√†n th√†nh: {len(movies)} phim")
                print(f"üíæ File: {result.get('file', '')}")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        import traceback
        print(f"\n‚ùå L·ªói: {str(e)}")
        traceback.print_exc()
    finally:
        await crawler.close_browser()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"‚ùå L·ªói ch∆∞∆°ng tr√¨nh: {e}")

