"""
Crawler Ä‘á»ƒ láº¥y áº£nh tá»« trang album cá»§a gaigu1.net.
Sá»­ dá»¥ng Playwright Ä‘á»ƒ táº£i trang vÃ  trÃ­ch xuáº¥t táº¥t cáº£ áº£nh trong album.
"""

import asyncio
import json
import os
import random
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

from playwright.async_api import async_playwright, Browser


class AlbumCrawler:
    def __init__(
        self,
        delay_min: float = 1.0,
        delay_max: float = 2.5,
        headless: bool = False,  # má»Ÿ Chrome tháº­t khi cáº§n debug
    ):
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.headless = headless

    async def init_browser(self):
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless, args=["--no-sandbox", "--disable-setuid-sandbox"]
            )
        return self.browser

    async def close_browser(self):
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi Ä‘Ã³ng browser: {e}")

    async def crawl_album_images(self, url: str) -> Dict:
        """
        Crawl táº¥t cáº£ áº£nh tá»« trang album.
        
        Args:
            url: URL cá»§a trang album cáº§n crawl
            
        Returns:
            Dict chá»©a thÃ´ng tin album vÃ  danh sÃ¡ch áº£nh

        Args:
            url: URL cá»§a trang album (vÃ­ dá»¥: https://gaigu1.net/album-anh-sex/24062/...)

        Returns:
            Dict chá»©a thÃ´ng tin album vÃ  danh sÃ¡ch áº£nh
        """
        await self.init_browser()
        page = await self.browser.new_page()
        await page.set_extra_http_headers(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://gaigu1.net/",
            }
        )

        try:
            print(f"ğŸ” Äang crawl album: {url}")
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(1)  # Giáº£m thá»i gian chá»

            # Láº¥y thÃ´ng tin album (title, description, etc.)
            album_info = await page.evaluate(
                """
                () => {
                    const title = document.querySelector('h1')?.textContent?.trim() || 
                                  document.querySelector('.album-title')?.textContent?.trim() || 
                                  document.querySelector('title')?.textContent?.trim() || '';
                    
                    const description = document.querySelector('.album-description')?.textContent?.trim() || 
                                      document.querySelector('.description')?.textContent?.trim() || '';
                    
                    return { title, description };
                }
                """
            )

            # Thá»­ nhiá»u cÃ¡ch Ä‘á»ƒ láº¥y áº£nh
            # 1. TÃ¬m trong gallery/swiper
            # 2. TÃ¬m táº¥t cáº£ img cÃ³ data-src hoáº·c src
            # 3. TÃ¬m trong cÃ¡c container phá»• biáº¿n
            images = await page.evaluate(
                """
                () => {
                    const imageUrls = new Set();
                    const baseUrl = window.location.origin;

                    // CÃ¡ch 1: TÃ¬m trong gallery/swiper
                    const gallerySelectors = [
                        '.gallery img',
                        '.swiper-wrapper img',
                        '.album-gallery img',
                        '.photo-gallery img',
                        '.image-gallery img',
                        '[class*="gallery"] img',
                        '[class*="swiper"] img',
                    ];

                    gallerySelectors.forEach(selector => {
                        document.querySelectorAll(selector).forEach(img => {
                            const src = img.getAttribute('data-src') || 
                                       img.getAttribute('data-lazy') ||
                                       img.getAttribute('src') || '';
                            if (src && !src.startsWith('data:')) {
                                const fullUrl = src.startsWith('http') ? src : new URL(src, baseUrl).href;
                                imageUrls.add(fullUrl);
                            }
                        });
                    });

                    // CÃ¡ch 2: TÃ¬m táº¥t cáº£ img trong cÃ¡c container chÃ­nh
                    const mainContainers = [
                        '.album-content',
                        '.album-images',
                        '.content',
                        '.main-content',
                        '[class*="album"]',
                    ];

                    mainContainers.forEach(containerSelector => {
                        const container = document.querySelector(containerSelector);
                        if (container) {
                            container.querySelectorAll('img').forEach(img => {
                                const src = img.getAttribute('data-src') || 
                                           img.getAttribute('data-lazy') ||
                                           img.getAttribute('src') || '';
                                if (src && !src.startsWith('data:') && !src.includes('logo') && !src.includes('icon')) {
                                    const fullUrl = src.startsWith('http') ? src : new URL(src, baseUrl).href;
                                    imageUrls.add(fullUrl);
                                }
                            });
                        }
                    });

                    // CÃ¡ch 3: TÃ¬m táº¥t cáº£ img trÃªn trang (fallback)
                    if (imageUrls.size === 0) {
                        document.querySelectorAll('img').forEach(img => {
                            const src = img.getAttribute('data-src') || 
                                       img.getAttribute('data-lazy') ||
                                       img.getAttribute('src') || '';
                            if (src && !src.startsWith('data:') && 
                                !src.includes('logo') && 
                                !src.includes('icon') &&
                                !src.includes('avatar') &&
                                img.offsetWidth > 100 &&  // Lá»c áº£nh nhá» (icon, avatar)
                                img.offsetHeight > 100) {
                                const fullUrl = src.startsWith('http') ? src : new URL(src, baseUrl).href;
                                imageUrls.add(fullUrl);
                            }
                        });
                    }

                    return Array.from(imageUrls);
                }
                """
            )

            # Thá»­ scroll Ä‘á»ƒ load thÃªm áº£nh lazy load
            if len(images) < 10:  # Náº¿u cÃ³ Ã­t áº£nh, thá»­ scroll
                print("ğŸ“œ Äang scroll Ä‘á»ƒ load thÃªm áº£nh...")
                for i in range(3):  # Giáº£m sá»‘ láº§n scroll
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                    await page.wait_for_timeout(1000)  # Giáº£m thá»i gian chá»
                    
                    # Láº¥y láº¡i danh sÃ¡ch áº£nh sau khi scroll
                    new_images = await page.evaluate(
                        """
                        () => {
                            const imageUrls = new Set();
                            const baseUrl = window.location.origin;
                            document.querySelectorAll('img').forEach(img => {
                                const src = img.getAttribute('data-src') || 
                                           img.getAttribute('data-lazy') ||
                                           img.getAttribute('src') || '';
                                if (src && !src.startsWith('data:') && 
                                    !src.includes('logo') && 
                                    !src.includes('icon') &&
                                    !src.includes('avatar') &&
                                    img.offsetWidth > 100 &&
                                    img.offsetHeight > 100) {
                                    const fullUrl = src.startsWith('http') ? src : new URL(src, baseUrl).href;
                                    imageUrls.add(fullUrl);
                                }
                            });
                            return Array.from(imageUrls);
                        }
                        """
                    )
                    if len(new_images) > len(images):
                        images = new_images
                        print(f"  ğŸ“¸ TÃ¬m tháº¥y {len(images)} áº£nh sau scroll {i+1}")
                    else:
                        break

            # Lá»c vÃ  chuáº©n hÃ³a URL
            cleaned_images = []
            for img_url in images:
                # Loáº¡i bá» cÃ¡c tham sá»‘ resize/crop náº¿u cÃ³
                cleaned_url = img_url.split('?')[0] if '?' in img_url else img_url
                if cleaned_url not in cleaned_images:
                    cleaned_images.append(cleaned_url)

            result = {
                "url": url,
                "title": album_info.get("title", ""),
                "description": album_info.get("description", ""),
                "images": cleaned_images,
                "total_images": len(cleaned_images),
                "crawled_at": datetime.now().isoformat(),
            }

            print(f"âœ… Thu Ä‘Æ°á»£c {len(cleaned_images)} áº£nh tá»« album")
            return result

        except Exception as e:
            print(f"âŒ Lá»—i crawl album {url}: {e}")
            return {
                "url": url,
                "error": str(e),
                "images": [],
                "total_images": 0,
            }
        finally:
            await page.close()

    def save_to_json(self, data: Dict, filename: Optional[str] = None, output_folder: Optional[str] = None) -> str:
        """LÆ°u dá»¯ liá»‡u album vÃ o file JSON. Má»—i album = 1 file JSON riÃªng."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # Táº¡o tÃªn file tá»« title hoáº·c URL
            title = data.get("title", "")
            if title:
                # LÃ m sáº¡ch title Ä‘á»ƒ lÃ m tÃªn file
                import re
                safe_title = re.sub(r'[^\w\s-]', '', title).strip()[:50]
                safe_title = re.sub(r'[-\s]+', '-', safe_title)
                filename = f"album_{safe_title}_{timestamp}.json"
            else:
                # Fallback: dÃ¹ng URL
                url_part = data.get("url", "").split("/")[-1][:50] if data.get("url") else "album"
                filename = f"album_{url_part}_{timestamp}.json"

        # Sá»­ dá»¥ng output_folder náº¿u Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh, náº¿u khÃ´ng thÃ¬ dÃ¹ng folder data máº·c Ä‘á»‹nh
        if output_folder:
            data_dir = os.path.join(os.path.dirname(__file__), "data", output_folder)
        else:
            data_dir = os.path.join(os.path.dirname(__file__), "data")
        os.makedirs(data_dir, exist_ok=True)
        filepath = os.path.join(data_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ÄÃ£ lÆ°u album vÃ o: {filepath}")
        return filepath

    async def crawl_single_album(self, url: str, idx: int, total: int, output_folder: str) -> Dict:
        """Crawl má»™t album Ä‘Æ¡n láº»."""
        try:
            print(f"ğŸ“¦ Album {idx}/{total}: {url}")
            album_data = await self.crawl_album_images(url)
            
            if album_data.get("error"):
                print(f"âŒ Lá»—i khi crawl album {idx}: {album_data.get('error')}")
                return {
                    "url": url,
                    "success": False,
                    "error": album_data.get("error"),
                }
            else:
                filepath = self.save_to_json(album_data, output_folder=output_folder)
                return {
                    "url": url,
                    "success": True,
                    "filepath": filepath,
                    "total_images": album_data.get("total_images", 0),
                }
        except Exception as e:
            print(f"âŒ Lá»—i khi crawl album {idx}: {e}")
            return {
                "url": url,
                "success": False,
                "error": str(e),
            }

    async def crawl_multiple_albums(self, urls: List[str], output_folder: Optional[str] = None, max_concurrent: int = 3) -> List[Dict]:
        """
        Crawl nhiá»u album, má»—i album lÆ°u thÃ nh 1 file JSON riÃªng.
        Há»— trá»£ crawl Ä‘á»“ng thá»i Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™.
        
        Args:
            urls: Danh sÃ¡ch URL cÃ¡c album cáº§n crawl
            output_folder: TÃªn folder Ä‘á»ƒ lÆ°u cÃ¡c album (tÃ¹y chá»n)
            max_concurrent: Sá»‘ lÆ°á»£ng album crawl Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh: 3)
            
        Returns:
            Danh sÃ¡ch káº¿t quáº£ crawl cá»§a tá»«ng album
        """
        total = len(urls)
        
        # Táº¡o folder riÃªng náº¿u chÆ°a cÃ³
        if not output_folder:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_folder = f"albums_batch_{timestamp}"
        
        output_path = os.path.join(os.path.dirname(__file__), "data", output_folder)
        os.makedirs(output_path, exist_ok=True)
        print(f"\nğŸ“ LÆ°u album vÃ o folder: {output_folder}\n")
        print(f"ğŸš€ Báº¯t Ä‘áº§u crawl {total} album(s) vá»›i {max_concurrent} luá»“ng Ä‘á»“ng thá»i...\n")
        
        # Táº¡o semaphore Ä‘á»ƒ giá»›i háº¡n sá»‘ lÆ°á»£ng crawl Ä‘á»“ng thá»i
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_with_semaphore(url: str, idx: int):
            async with semaphore:
                # Delay ngáº«u nhiÃªn nhá» Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i
                if idx > 1:
                    delay = random.uniform(self.delay_min, self.delay_max)
                    await asyncio.sleep(delay)
                return await self.crawl_single_album(url, idx, total, output_folder)
        
        # Táº¡o táº¥t cáº£ tasks
        tasks = [crawl_with_semaphore(url, idx) for idx, url in enumerate(urls, 1)]
        
        # Cháº¡y táº¥t cáº£ tasks vÃ  Ä‘á»£i káº¿t quáº£
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Xá»­ lÃ½ exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "url": urls[i],
                    "success": False,
                    "error": str(result),
                })
            else:
                processed_results.append(result)
        
        return processed_results


async def main():
    import sys

    # Nháº­n nhiá»u URL tá»« command line arguments
    # CÃ¡ch 1: Truyá»n nhiá»u URL lÃ m tham sá»‘
    # python album_crawler.py "url1" "url2" "url3"
    urls = []
    
    if len(sys.argv) > 1:
        # Náº¿u tham sá»‘ Ä‘áº§u tiÃªn lÃ  file (cÃ³ extension .txt hoáº·c .json)
        first_arg = sys.argv[1]
        if first_arg.endswith('.txt') or first_arg.endswith('.json'):
            # Äá»c danh sÃ¡ch URL tá»« file
            try:
                with open(first_arg, 'r', encoding='utf-8') as f:
                    if first_arg.endswith('.json'):
                        # Náº¿u lÃ  JSON, Ä‘á»c nhÆ° array
                        data = json.load(f)
                        if isinstance(data, list):
                            urls = data
                        elif isinstance(data, dict) and 'urls' in data:
                            urls = data['urls']
                    else:
                        # Náº¿u lÃ  TXT, Ä‘á»c tá»«ng dÃ²ng
                        urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
                print(f"ğŸ“„ ÄÃ£ Ä‘á»c {len(urls)} URL tá»« file: {first_arg}")
            except Exception as e:
                print(f"âŒ Lá»—i Ä‘á»c file {first_arg}: {e}")
                return
        else:
            # Láº¥y táº¥t cáº£ tham sá»‘ lÃ m URL
            urls = sys.argv[1:]
    else:
        # URL máº·c Ä‘á»‹nh Ä‘á»ƒ test
        urls = ["https://gaigu1.net/album-anh-sex/24062/m%E1%BB%B9-anh-t%C3%A2y-ninh-nyc-b%E1%BB%93n-ch%E1%BB%A9a-tinh-n%C4%83m-c3"]

    if not urls:
        print("âŒ KhÃ´ng cÃ³ URL nÃ o Ä‘á»ƒ crawl!")
        print("\nCÃ¡ch sá»­ dá»¥ng:")
        print("  python album_crawler.py <url1> <url2> <url3> ...")
        print("  python album_crawler.py urls.txt  # Äá»c tá»« file txt (má»—i dÃ²ng 1 URL)")
        print("  python album_crawler.py urls.json  # Äá»c tá»« file json (array URLs)")
        return

    # Báº­t headless mode vÃ  giáº£m delay Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
    crawler = AlbumCrawler(headless=True, delay_min=0.5, delay_max=1.0)
    try:
        # Táº¡o tÃªn folder riÃªng cho batch nÃ y
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_folder = f"albums_batch_{timestamp}"
        
        # Crawl vá»›i 5 luá»“ng Ä‘á»“ng thá»i Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
        results = await crawler.crawl_multiple_albums(urls, output_folder=output_folder, max_concurrent=5)
        
        # Tá»•ng káº¿t
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Tá»”NG Káº¾T")
        print(f"{'='*60}")
        success_count = sum(1 for r in results if r.get("success"))
        total_images = sum(r.get("total_images", 0) for r in results if r.get("success"))
        
        print(f"âœ… ThÃ nh cÃ´ng: {success_count}/{len(results)} album")
        print(f"ğŸ“¸ Tá»•ng sá»‘ áº£nh: {total_images}")
        print(f"âŒ Tháº¥t báº¡i: {len(results) - success_count} album")
        print(f"\nğŸ“ Táº¥t cáº£ album Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o folder: crawler/data/{output_folder}/")
        
        if success_count > 0:
            print(f"\nğŸ“„ Má»™t sá»‘ file Ä‘Ã£ lÆ°u (hiá»ƒn thá»‹ 5 file Ä‘áº§u):")
            for r in results[:5]:
                if r.get("success"):
                    print(f"  - {os.path.basename(r.get('filepath'))} ({r.get('total_images', 0)} áº£nh)")
            if success_count > 5:
                print(f"  ... vÃ  {success_count - 5} file khÃ¡c")
        
    finally:
        await crawler.close_browser()


if __name__ == "__main__":
    asyncio.run(main())

